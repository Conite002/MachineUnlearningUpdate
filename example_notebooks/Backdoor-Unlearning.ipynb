{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backdoor Unlearning\n",
    "\n",
    "## Outline\n",
    "\n",
    "1. Experimental setup (generating configs)\n",
    "2. Clean model training\n",
    "3. Poisoned model training\n",
    "4. First-order unlearning\n",
    "5. Second-order unlearning\n",
    "6. Visualizing results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Setup\n",
    "\n",
    "- All configurations to test are defined in the `[train|poison|unlearn].json` files (see below).\n",
    "- If parameters are passed as list, all their combinations are tested in a grid-search manner.\n",
    "- Only a single combination is provided for this demo. The original combinations are in `Applications/Poisoning/configs`\n",
    "- The function generates directories and configuration files for each combination. They are later used by an evaluation script to run the experiment. This allows for parallelization and distributed execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only if you are using CUDA devices\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conf import BASE_DIR\n",
    "from Applications.Poisoning.gen_configs import main as gen_configs\n",
    "\n",
    "model_folder = BASE_DIR/'models'/'poisoning'\n",
    "train_conf = BASE_DIR/'Applications'/'Poisoning'/'configs'/'demo'/'train.json'\n",
    "poison_conf = BASE_DIR/'Applications'/'Poisoning'/'configs'/'demo'/'poison.json'\n",
    "unlearn_conf = BASE_DIR/'Applications'/'Poisoning'/'configs'/'demo'/'unlearn.json'\n",
    "\n",
    "gen_configs(model_folder, train_conf, poison_conf, unlearn_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Applications.Poisoning.poison.poison_models import train_poisoned\n",
    "from Applications.Poisoning.configs.demo.config import Config\n",
    "\n",
    "poisoned_folder = model_folder/'budget-10000'/'seed-42'\n",
    "clean_folder = model_folder/'clean'\n",
    "first_unlearn_folder = model_folder/'budget-10000'/'seed-42'/'first-order'\n",
    "second_unlearn_folder = model_folder/'budget-10000'/'seed-42'/'second-order'\n",
    "\n",
    "\n",
    "poison_kwargs = Config.from_json(poisoned_folder/'poison_config.json')\n",
    "train_kwargs = Config.from_json(poisoned_folder/'train_config.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Model Training\n",
    "\n",
    "- Train a clean model for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Poisoned Model\n",
    "\n",
    "- Select one of the generated configurations and train a poisoned model.\n",
    "- The poisoning uses an `injector` object which can be persisted for reproducibility. It will inject the backdoors/label noise into the same samples according to a seed. In our experiments, we worked with label noise poisoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Applications.Poisoning.poison.poison_models import train_poisoned\n",
    "from Applications.Poisoning.configs.demo.config import Config\n",
    "\n",
    "poisoned_folder = model_folder/'budget-10000'/'seed-42'\n",
    "clean_folder = model_folder/'clean'\n",
    "first_unlearn_folder = model_folder/'budget-10000'/'seed-42'/'first-order'\n",
    "second_unlearn_folder = model_folder/'budget-10000'/'seed-42'/'second-order'\n",
    "\n",
    "\n",
    "poison_kwargs = Config.from_json(poisoned_folder/'poison_config.json')\n",
    "train_kwargs = Config.from_json(poisoned_folder/'train_config.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisoned_weights = poisoned_folder/'best_model.hdf5'       # model that has been trained on poisoned data\n",
    "fo_repaired_weights = poisoned_folder/'fo_repaired.hdf5'   # model weights after unlearning (first-order)\n",
    "so_repaired_weights = poisoned_folder/'so_repaired.hdf5'   # model weights after unlearning (second-order)\n",
    "injector_path = poisoned_folder/'injector.pkl'             # cached injector for reproducibility\n",
    "clean_results = model_folder/'clean'/'train_results.json'  # path to reference results on clean dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unlearning\n",
    "\n",
    "- Perform the first-order and second-order unlearning. The unlearning is wrapped in a function that\n",
    "    - loads the clean data, saves the original labels\n",
    "    - injects the poison (label noise)\n",
    "    - creates difference set Z using `injector.injected_idx`\n",
    "    - main unlearning happens in `Applications.Poisoning.unlearn.common.py:unlearn_update` and the thereby called `iter_approx_retraining` method\n",
    "- The variable naming follows the following ideas:\n",
    "    - `z_x`, `z_y`: features (x) and labels (y) in set `Z`\n",
    "    - `z_x_delta`, `z_y_delta`: changed features and labels (`z_x == z_x_delta` here and `z_y_delta` contains the original (fixed) labels)\n",
    "- A word about why iterative:\n",
    "    - The approximate retraining is configured to unlearn the desired changes in one step.\n",
    "    - To avoid putting a lot of redundant erroneous samples in the changing set `Z`, the iterative version\n",
    "        - takes a sub-sample (`prio_idx`) of `hvp_batch_size` in the delta set `Z`\n",
    "        - makes one unlearning step\n",
    "        - recalculates the delta set and focuses only on remaining errors\n",
    "    - The idea here is that similar to learning, it is better to work iteratively in batches since the approximation quality of the inverse hessian matrix decreases with the number of samples included (and the step size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Applications.Poisoning.unlearn.first_order import run_experiment as fo_experiment\n",
    "from Applications.Poisoning.unlearn.second_order import run_experiment as so_experiment\n",
    "\n",
    "fo_unlearn_kwargs = Config.from_json(poisoned_folder/'first-order'/'unlearn_config.json')\n",
    "so_unlearn_kwargs = Config.from_json(poisoned_folder/'second-order'/'unlearn_config.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Applications.Poisoning.train import main as train\n",
    "from Applications.Poisoning.evaluate import evaluate\n",
    "\n",
    "# train one clean and one poisoned model\n",
    "# datasets = ['Cifar10', 'Cifar100', 'SVHN', 'FashionMNIST']\n",
    "datasets = ['Cifar100', 'SVHN', 'FashionMNIST']\n",
    "modelnames = ['VGG16', 'RESNET50', 'extractfeatures_VGG16', 'extractfeatures_RESNET50', 'classifier_VGG16', 'classifier_RESNET50']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "results = {\n",
    "    'clean': {},\n",
    "    'poisoned': {},\n",
    "    'first_order_unlearning': {},\n",
    "    'second_order_unlearning': {}\n",
    "}\n",
    "\n",
    "for dataset in datasets:\n",
    "    results['clean'][dataset] = {}\n",
    "    results['poisoned'][dataset] = {}\n",
    "    results['first_order_unlearning'][dataset] = {}\n",
    "    results['second_order_unlearning'][dataset] = {}\n",
    "    \n",
    "    for modelname in modelnames:\n",
    "        print('*' * 40)\n",
    "        print(f\"* Training {modelname} on {dataset} started. *\")\n",
    "        print('*' * 40)\n",
    "        train(model_folder=model_folder/'clean', dataset=dataset, modelname=modelname)\n",
    "        print('*' * 40)\n",
    "        print(f\"* Training {modelname} on {dataset} done. *\")\n",
    "        print('*' * 40)\n",
    "        clean_accuracy = evaluate(model_folder=model_folder/'clean', dataset=dataset, modelname=modelname, type='best')\n",
    "        results['clean'][dataset][modelname] = clean_accuracy\n",
    "\n",
    "    print('#' * 40)\n",
    "    print(f\"################ POISONING ################\")\n",
    "    print('#' * 40)\n",
    "    for modelname in modelnames:\n",
    "        print('*' * 40)\n",
    "        print(f\"* Poisoning {modelname} on {dataset} started. *\")\n",
    "        print('*' * 40)\n",
    "        train_poisoned(model_folder=poisoned_folder, poison_kwargs=poison_kwargs, train_kwargs=train_kwargs, dataset=dataset, modelname=modelname)\n",
    "        print('*' * 40)\n",
    "        print(f\"* Poisoning {modelname} on {dataset} done. *\")\n",
    "        print('*' * 40)\n",
    "        poisoned_accuracy = evaluate(model_folder=poisoned_folder, dataset=dataset, modelname=modelname, type='poisoned')\n",
    "        results['poisoned'][dataset][modelname] = poisoned_accuracy\n",
    "\n",
    "    # unlearn the poisoned model\n",
    "    print('#' * 40)\n",
    "    print(f\"################ UNLEARNING ################\")\n",
    "    print('#' * 40)\n",
    "\n",
    "    for modelname in modelnames:\n",
    "        print('*' * 40)\n",
    "        print(f\"* Evaluating {modelname} on {dataset} poisoned model *\")\n",
    "        print('*' * 40)\n",
    "        poisoned_accuracy = evaluate(model_folder=poisoned_folder, dataset=dataset, modelname=modelname, type='poisoned')\n",
    "        results['poisoned'][dataset][modelname] = poisoned_accuracy\n",
    "        \n",
    "        print('*' * 40)\n",
    "        print(f\"* First-order unlearning {modelname} on {dataset} poisoned model *\")\n",
    "        print('*' * 40)\n",
    "        try:\n",
    "            fo_experiment(poisoned_folder/'first-order', train_kwargs, poison_kwargs, fo_unlearn_kwargs, dataset=dataset, modelname=modelname)\n",
    "        \n",
    "            print('*' * 40)\n",
    "            print(f\"* Evaluating {modelname} on {dataset} after first-order unlearning *\")\n",
    "            print('*' * 40)\n",
    "            fo_repaired_accuracy = evaluate(model_folder=first_unlearn_folder, dataset=dataset, modelname=modelname, type='repaired')\n",
    "            results['first_order_unlearning'][dataset][modelname] = fo_repaired_accuracy\n",
    "        except Exception as e:\n",
    "            print(f\"Error during first-order unlearning for {modelname} on {dataset}: {e}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        print('*' * 40)\n",
    "        print(f\"* Second-order unlearning {modelname} on {dataset} poisoned model *\")\n",
    "        print('*' * 40)\n",
    "        try:\n",
    "            so_experiment(poisoned_folder/'second-order', train_kwargs, poison_kwargs, so_unlearn_kwargs, dataset=dataset, modelname=modelname)\n",
    "\n",
    "            print('*' * 40)\n",
    "            print(f\"* Evaluating {modelname} on {dataset} after second-order unlearning *\")\n",
    "            print('*' * 40)\n",
    "            so_repaired_accuracy = evaluate(model_folder=second_unlearn_folder, dataset=dataset, modelname=modelname, type='repaired')\n",
    "            results['second_order_unlearning'][dataset][modelname] = so_repaired_accuracy\n",
    "        except Exception as e:\n",
    "            print(f\"Error during second-order unlearning for {modelname} on {dataset}: {e}\")\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the results dictionary to a pandas DataFrame for easy plotting\n",
    "data = []\n",
    "\n",
    "for phase in results:\n",
    "    for dataset in results[phase]:\n",
    "        for modelname in results[phase][dataset]:\n",
    "            data.append({\n",
    "                'Phase': phase,\n",
    "                'Dataset': dataset,\n",
    "                'Model': modelname,\n",
    "                'Accuracy': results[phase][dataset][modelname]\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create a seaborn barplot to visualize the accuracy of each model in each phase\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Dataset', y='Accuracy', hue='Phase', data=df)\n",
    "plt.title('Model Accuracy Across Different Phases and Datasets')\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(title='Phase')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
