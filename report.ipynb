{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La perplexitÃ© est une mesure couramment utilisÃ©e en traitement du langage naturel (NLP) pour Ã©valuer la qualitÃ© d'un modÃ¨le probabiliste Ã  prÃ©dire une sÃ©quence de mots. Dans le contexte du dÃ©sapprentissage, cette mesure peut Ãªtre adaptÃ©e pour Ã©valuer la performance du modÃ¨le aprÃ¨s que certaines donnÃ©es ont Ã©tÃ© supprimÃ©es. Voici comment on pourrait l'adapter :\n",
    "\n",
    "Formule de la PerplexitÃ©\n",
    "La perplexitÃ© (\n",
    "ğ‘ƒ\n",
    "ğ‘ƒ\n",
    "ğ¿\n",
    "PPL) est calculÃ©e Ã  partir de la probabilitÃ© des prÃ©dictions du modÃ¨le :\n",
    "\n",
    "ğ‘ƒ\n",
    "ğ‘ƒ\n",
    "ğ¿\n",
    "=\n",
    "exp\n",
    "â¡\n",
    "(\n",
    "âˆ’\n",
    "1\n",
    "ğ‘\n",
    "âˆ‘\n",
    "ğ‘–\n",
    "=\n",
    "1\n",
    "ğ‘\n",
    "log\n",
    "â¡\n",
    "ğ‘ƒ\n",
    "(\n",
    "ğ‘¥\n",
    "ğ‘–\n",
    ")\n",
    ")\n",
    "PPL=exp(âˆ’ \n",
    "N\n",
    "1\n",
    "â€‹\n",
    " âˆ‘ \n",
    "i=1\n",
    "N\n",
    "â€‹\n",
    " logP(x \n",
    "i\n",
    "â€‹\n",
    " ))\n",
    "\n",
    "oÃ¹ :\n",
    "\n",
    "ğ‘\n",
    "N est le nombre total de mots (ou Ã©chantillons) dans le jeu de test.\n",
    "ğ‘ƒ\n",
    "(\n",
    "ğ‘¥\n",
    "ğ‘–\n",
    ")\n",
    "P(x \n",
    "i\n",
    "â€‹\n",
    " ) est la probabilitÃ© attribuÃ©e par le modÃ¨le au \n",
    "ğ‘–\n",
    "i-Ã¨me mot (ou Ã©chantillon).\n",
    "Adaptation au DÃ©sapprentissage\n",
    "Avant le DÃ©sapprentissage :\n",
    "\n",
    "EntraÃ®ner le modÃ¨le avec les donnÃ©es complÃ¨tes.\n",
    "Calculer la perplexitÃ© sur un jeu de test complet.\n",
    "AprÃ¨s le DÃ©sapprentissage :\n",
    "\n",
    "Appliquer les techniques de dÃ©sapprentissage pour \"oublier\" certaines donnÃ©es.\n",
    "RÃ©entraÃ®ner ou ajuster le modÃ¨le.\n",
    "Calculer la nouvelle perplexitÃ© sur le mÃªme jeu de test.\n",
    "Comparaison des PerplexitÃ©s\n",
    "Comparer les valeurs de perplexitÃ© avant et aprÃ¨s le dÃ©sapprentissage pour Ã©valuer l'impact du dÃ©sapprentissage sur la capacitÃ© du modÃ¨le Ã  prÃ©dire correctement les donnÃ©es restantes.\n",
    "\n",
    "Exemple de Code\n",
    "python\n",
    "Copy code\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def calculate_perplexity(probs):\n",
    "    N = len(probs)\n",
    "    log_prob_sum = np.sum(np.log(probs))\n",
    "    perplexity = np.exp(-log_prob_sum / N)\n",
    "    return perplexity\n",
    "\n",
    "# PrÃ©dictions et probabilitÃ©s avant dÃ©sapprentissage\n",
    "probs_before = model.predict_proba(test_data[0])\n",
    "perplexity_before = calculate_perplexity(np.max(probs_before, axis=1))\n",
    "\n",
    "# PrÃ©dictions et probabilitÃ©s aprÃ¨s dÃ©sapprentissage\n",
    "probs_after = model_unlearned.predict_proba(test_data[0])\n",
    "perplexity_after = calculate_perplexity(np.max(probs_after, axis=1))\n",
    "Garantie pour le DÃ©sapprentissage\n",
    "Pour garantir que les donnÃ©es dÃ©sapprises ne sont plus influentes, on peut utiliser des techniques comme des certificats formels de dÃ©sapprentissage, des tests empiriques ou des audits externes.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
