{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backdoor Unlearning\n",
    "\n",
    "## Outline\n",
    "\n",
    "1. Experimental setup (generating configs)\n",
    "2. Clean model training\n",
    "3. Poisoned model training\n",
    "4. First-order unlearning\n",
    "5. Second-order unlearning\n",
    "6. Visualizing results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Setup\n",
    "\n",
    "- All configurations to test are defined in the `[train|poison|unlearn].json` files (see below).\n",
    "- If parameters are passed as list, all their combinations are tested in a grid-search manner.\n",
    "- Only a single combination is provided for this demo. The original combinations are in `Applications/Poisoning/configs`\n",
    "- The function generates directories and configuration files for each combination. They are later used by an evaluation script to run the experiment. This allows for parallelization and distributed execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only if you are using CUDA devices\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conf import BASE_DIR\n",
    "from Applications.Poisoning.gen_configs import main as gen_configs\n",
    "\n",
    "model_folder = BASE_DIR/'models'/'poisoning'\n",
    "train_conf = BASE_DIR/'Applications'/'Poisoning'/'configs'/'demo'/'train.json'\n",
    "poison_conf = BASE_DIR/'Applications'/'Poisoning'/'configs'/'demo'/'poison.json'\n",
    "unlearn_conf = BASE_DIR/'Applications'/'Poisoning'/'configs'/'demo'/'unlearn.json'\n",
    "\n",
    "gen_configs(model_folder, train_conf, poison_conf, unlearn_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-11 18:41:29.988905: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-12.2/lib64\n",
      "2024-07-11 18:41:29.988921: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from Applications.Poisoning.poison.poison_models import train_poisoned\n",
    "from Applications.Poisoning.configs.demo.config import Config\n",
    "\n",
    "poisoned_folder = model_folder/'budget-10000'/'seed-42'\n",
    "clean_folder = model_folder/'clean'\n",
    "first_unlearn_folder = model_folder/'budget-10000'/'seed-42'/'first-order'\n",
    "second_unlearn_folder = model_folder/'budget-10000'/'seed-42'/'second-order'\n",
    "\n",
    "\n",
    "poison_kwargs = Config.from_json(poisoned_folder/'poison_config.json')\n",
    "train_kwargs = Config.from_json(poisoned_folder/'train_config.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Model Training\n",
    "\n",
    "- Train a clean model for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Poisoned Model\n",
    "\n",
    "- Select one of the generated configurations and train a poisoned model.\n",
    "- The poisoning uses an `injector` object which can be persisted for reproducibility. It will inject the backdoors/label noise into the same samples according to a seed. In our experiments, we worked with label noise poisoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Applications.Poisoning.poison.poison_models import train_poisoned\n",
    "from Applications.Poisoning.configs.demo.config import Config\n",
    "\n",
    "poisoned_folder = model_folder/'budget-10000'/'seed-42'\n",
    "clean_folder = model_folder/'clean'\n",
    "first_unlearn_folder = model_folder/'budget-10000'/'seed-42'/'first-order'\n",
    "second_unlearn_folder = model_folder/'budget-10000'/'seed-42'/'second-order'\n",
    "\n",
    "\n",
    "poison_kwargs = Config.from_json(poisoned_folder/'poison_config.json')\n",
    "train_kwargs = Config.from_json(poisoned_folder/'train_config.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisoned_weights = poisoned_folder/'best_model.hdf5'       # model that has been trained on poisoned data\n",
    "fo_repaired_weights = poisoned_folder/'fo_repaired.hdf5'   # model weights after unlearning (first-order)\n",
    "so_repaired_weights = poisoned_folder/'so_repaired.hdf5'   # model weights after unlearning (second-order)\n",
    "injector_path = poisoned_folder/'injector.pkl'             # cached injector for reproducibility\n",
    "clean_results = model_folder/'clean'/'train_results.json'  # path to reference results on clean dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unlearning\n",
    "\n",
    "- Perform the first-order and second-order unlearning. The unlearning is wrapped in a function that\n",
    "    - loads the clean data, saves the original labels\n",
    "    - injects the poison (label noise)\n",
    "    - creates difference set Z using `injector.injected_idx`\n",
    "    - main unlearning happens in `Applications.Poisoning.unlearn.common.py:unlearn_update` and the thereby called `iter_approx_retraining` method\n",
    "- The variable naming follows the following ideas:\n",
    "    - `z_x`, `z_y`: features (x) and labels (y) in set `Z`\n",
    "    - `z_x_delta`, `z_y_delta`: changed features and labels (`z_x == z_x_delta` here and `z_y_delta` contains the original (fixed) labels)\n",
    "- A word about why iterative:\n",
    "    - The approximate retraining is configured to unlearn the desired changes in one step.\n",
    "    - To avoid putting a lot of redundant erroneous samples in the changing set `Z`, the iterative version\n",
    "        - takes a sub-sample (`prio_idx`) of `hvp_batch_size` in the delta set `Z`\n",
    "        - makes one unlearning step\n",
    "        - recalculates the delta set and focuses only on remaining errors\n",
    "    - The idea here is that similar to learning, it is better to work iteratively in batches since the approximation quality of the inverse hessian matrix decreases with the number of samples included (and the step size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Applications.Poisoning.unlearn.first_order import run_experiment as fo_experiment\n",
    "from Applications.Poisoning.unlearn.second_order import run_experiment as so_experiment\n",
    "\n",
    "fo_unlearn_kwargs = Config.from_json(poisoned_folder/'first-order'/'unlearn_config.json')\n",
    "so_unlearn_kwargs = Config.from_json(poisoned_folder/'second-order'/'unlearn_config.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Applications.Poisoning.train import main as train\n",
    "from Applications.Poisoning.evaluate import evaluate\n",
    "\n",
    "# train one clean and one poisoned model\n",
    "# datasets = ['Cifar10', 'Cifar100', 'SVHN', 'FashionMnist']\n",
    "datasets = ['Cifar100', 'Cifar10', 'SVHN']\n",
    "# modelnames = ['extractfeatures_VGG16', 'classifier_VGG16']\n",
    "modelnames = ['VGG16', 'RESNET50']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      " UNLEARNING \n",
      "############################################################\n",
      "\n",
      "\n",
      "\n",
      "* Evaluating VGG16 on Cifar100 poisoned model *\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-11 18:41:35.433048: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2024-07-11 18:41:35.433065: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: conite-HP\n",
      "2024-07-11 18:41:35.433069: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: conite-HP\n",
      "2024-07-11 18:41:35.433153: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 555.42.2\n",
      "2024-07-11 18:41:35.433164: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 555.42.2\n",
      "2024-07-11 18:41:35.433167: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 555.42.2\n",
      "2024-07-11 18:41:35.433343: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from None\n",
      "Model weights loaded successfully from /home/conite/Documents/WORKSPACE/PROJECTS/Memoire M2/code/MachineUnlearningUpdate/models/poisoning/budget-10000/seed-42/Cifar100_VGG16_poisoned_model.hdf5.\n",
      "Accuracy Cifar100 model: 0.6082\n",
      "* First-order unlearning VGG16 on Cifar100 poisoned model *\n",
      "Loading weights from None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-11 18:41:44.928052: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 614400000 exceeds 10% of free system memory.\n",
      "2024-07-11 18:41:45.192172: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 536870912 exceeds 10% of free system memory.\n",
      "2024-07-11 18:41:45.252326: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 536870912 exceeds 10% of free system memory.\n",
      "2024-07-11 18:41:45.315803: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 536870912 exceeds 10% of free system memory.\n",
      "2024-07-11 18:41:45.389537: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 536870912 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 3, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 100)\n",
      "(100,)\n",
      "Updated 38 weights. 0 weights were not updated.\n",
      "Updated non-trainable weights: [(28, 'batch_normalization_4/moving_mean:0'), (29, 'batch_normalization_4/moving_variance:0'), (30, 'conv2d_5/kernel:0'), (31, 'conv2d_5/bias:0'), (32, 'batch_normalization_5/gamma:0'), (33, 'batch_normalization_5/beta:0'), (34, 'batch_normalization_5/moving_mean:0'), (35, 'batch_normalization_5/moving_variance:0'), (36, 'conv2d_6/kernel:0'), (37, 'conv2d_6/bias:0'), (38, 'batch_normalization_6/gamma:0'), (39, 'batch_normalization_6/beta:0'), (40, 'batch_normalization_6/moving_mean:0'), (41, 'batch_normalization_6/moving_variance:0'), (42, 'dense/kernel:0'), (43, 'dense/bias:0'), (44, 'batch_normalization_7/gamma:0'), (45, 'batch_normalization_7/beta:0'), (46, 'batch_normalization_7/moving_mean:0'), (47, 'batch_normalization_7/moving_variance:0'), (48, 'dense_1/kernel:0'), (49, 'dense_1/bias:0'), (50, 'batch_normalization_8/gamma:0'), (51, 'batch_normalization_8/beta:0'), (52, 'batch_normalization_8/moving_mean:0'), (53, 'batch_normalization_8/moving_variance:0'), (54, 'dense_2/kernel:0'), (55, 'dense_2/bias:0')]\n",
      "(3, 3, 3, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 100)\n",
      "(100,)\n",
      "Updated 38 weights. 0 weights were not updated.\n",
      "Updated non-trainable weights: [(28, 'batch_normalization_4/moving_mean:0'), (29, 'batch_normalization_4/moving_variance:0'), (30, 'conv2d_5/kernel:0'), (31, 'conv2d_5/bias:0'), (32, 'batch_normalization_5/gamma:0'), (33, 'batch_normalization_5/beta:0'), (34, 'batch_normalization_5/moving_mean:0'), (35, 'batch_normalization_5/moving_variance:0'), (36, 'conv2d_6/kernel:0'), (37, 'conv2d_6/bias:0'), (38, 'batch_normalization_6/gamma:0'), (39, 'batch_normalization_6/beta:0'), (40, 'batch_normalization_6/moving_mean:0'), (41, 'batch_normalization_6/moving_variance:0'), (42, 'dense/kernel:0'), (43, 'dense/bias:0'), (44, 'batch_normalization_7/gamma:0'), (45, 'batch_normalization_7/beta:0'), (46, 'batch_normalization_7/moving_mean:0'), (47, 'batch_normalization_7/moving_variance:0'), (48, 'dense_1/kernel:0'), (49, 'dense_1/bias:0'), (50, 'batch_normalization_8/gamma:0'), (51, 'batch_normalization_8/beta:0'), (52, 'batch_normalization_8/moving_mean:0'), (53, 'batch_normalization_8/moving_variance:0'), (54, 'dense_2/kernel:0'), (55, 'dense_2/bias:0')]\n",
      "(3, 3, 3, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 100)\n",
      "(100,)\n",
      "Updated 38 weights. 0 weights were not updated.\n",
      "Updated non-trainable weights: [(28, 'batch_normalization_4/moving_mean:0'), (29, 'batch_normalization_4/moving_variance:0'), (30, 'conv2d_5/kernel:0'), (31, 'conv2d_5/bias:0'), (32, 'batch_normalization_5/gamma:0'), (33, 'batch_normalization_5/beta:0'), (34, 'batch_normalization_5/moving_mean:0'), (35, 'batch_normalization_5/moving_variance:0'), (36, 'conv2d_6/kernel:0'), (37, 'conv2d_6/bias:0'), (38, 'batch_normalization_6/gamma:0'), (39, 'batch_normalization_6/beta:0'), (40, 'batch_normalization_6/moving_mean:0'), (41, 'batch_normalization_6/moving_variance:0'), (42, 'dense/kernel:0'), (43, 'dense/bias:0'), (44, 'batch_normalization_7/gamma:0'), (45, 'batch_normalization_7/beta:0'), (46, 'batch_normalization_7/moving_mean:0'), (47, 'batch_normalization_7/moving_variance:0'), (48, 'dense_1/kernel:0'), (49, 'dense_1/bias:0'), (50, 'batch_normalization_8/gamma:0'), (51, 'batch_normalization_8/beta:0'), (52, 'batch_normalization_8/moving_mean:0'), (53, 'batch_normalization_8/moving_variance:0'), (54, 'dense_2/kernel:0'), (55, 'dense_2/bias:0')]\n",
      "(3, 3, 3, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 100)\n",
      "(100,)\n",
      "Updated 38 weights. 0 weights were not updated.\n",
      "Updated non-trainable weights: [(28, 'batch_normalization_4/moving_mean:0'), (29, 'batch_normalization_4/moving_variance:0'), (30, 'conv2d_5/kernel:0'), (31, 'conv2d_5/bias:0'), (32, 'batch_normalization_5/gamma:0'), (33, 'batch_normalization_5/beta:0'), (34, 'batch_normalization_5/moving_mean:0'), (35, 'batch_normalization_5/moving_variance:0'), (36, 'conv2d_6/kernel:0'), (37, 'conv2d_6/bias:0'), (38, 'batch_normalization_6/gamma:0'), (39, 'batch_normalization_6/beta:0'), (40, 'batch_normalization_6/moving_mean:0'), (41, 'batch_normalization_6/moving_variance:0'), (42, 'dense/kernel:0'), (43, 'dense/bias:0'), (44, 'batch_normalization_7/gamma:0'), (45, 'batch_normalization_7/beta:0'), (46, 'batch_normalization_7/moving_mean:0'), (47, 'batch_normalization_7/moving_variance:0'), (48, 'dense_1/kernel:0'), (49, 'dense_1/bias:0'), (50, 'batch_normalization_8/gamma:0'), (51, 'batch_normalization_8/beta:0'), (52, 'batch_normalization_8/moving_mean:0'), (53, 'batch_normalization_8/moving_variance:0'), (54, 'dense_2/kernel:0'), (55, 'dense_2/bias:0')]\n",
      "(3, 3, 3, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 100)\n",
      "(100,)\n",
      "Updated 38 weights. 0 weights were not updated.\n",
      "Updated non-trainable weights: [(28, 'batch_normalization_4/moving_mean:0'), (29, 'batch_normalization_4/moving_variance:0'), (30, 'conv2d_5/kernel:0'), (31, 'conv2d_5/bias:0'), (32, 'batch_normalization_5/gamma:0'), (33, 'batch_normalization_5/beta:0'), (34, 'batch_normalization_5/moving_mean:0'), (35, 'batch_normalization_5/moving_variance:0'), (36, 'conv2d_6/kernel:0'), (37, 'conv2d_6/bias:0'), (38, 'batch_normalization_6/gamma:0'), (39, 'batch_normalization_6/beta:0'), (40, 'batch_normalization_6/moving_mean:0'), (41, 'batch_normalization_6/moving_variance:0'), (42, 'dense/kernel:0'), (43, 'dense/bias:0'), (44, 'batch_normalization_7/gamma:0'), (45, 'batch_normalization_7/beta:0'), (46, 'batch_normalization_7/moving_mean:0'), (47, 'batch_normalization_7/moving_variance:0'), (48, 'dense_1/kernel:0'), (49, 'dense_1/bias:0'), (50, 'batch_normalization_8/gamma:0'), (51, 'batch_normalization_8/beta:0'), (52, 'batch_normalization_8/moving_mean:0'), (53, 'batch_normalization_8/moving_variance:0'), (54, 'dense_2/kernel:0'), (55, 'dense_2/bias:0')]\n",
      "(3, 3, 3, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 100)\n",
      "(100,)\n",
      "Updated 38 weights. 0 weights were not updated.\n",
      "Updated non-trainable weights: [(28, 'batch_normalization_4/moving_mean:0'), (29, 'batch_normalization_4/moving_variance:0'), (30, 'conv2d_5/kernel:0'), (31, 'conv2d_5/bias:0'), (32, 'batch_normalization_5/gamma:0'), (33, 'batch_normalization_5/beta:0'), (34, 'batch_normalization_5/moving_mean:0'), (35, 'batch_normalization_5/moving_variance:0'), (36, 'conv2d_6/kernel:0'), (37, 'conv2d_6/bias:0'), (38, 'batch_normalization_6/gamma:0'), (39, 'batch_normalization_6/beta:0'), (40, 'batch_normalization_6/moving_mean:0'), (41, 'batch_normalization_6/moving_variance:0'), (42, 'dense/kernel:0'), (43, 'dense/bias:0'), (44, 'batch_normalization_7/gamma:0'), (45, 'batch_normalization_7/beta:0'), (46, 'batch_normalization_7/moving_mean:0'), (47, 'batch_normalization_7/moving_variance:0'), (48, 'dense_1/kernel:0'), (49, 'dense_1/bias:0'), (50, 'batch_normalization_8/gamma:0'), (51, 'batch_normalization_8/beta:0'), (52, 'batch_normalization_8/moving_mean:0'), (53, 'batch_normalization_8/moving_variance:0'), (54, 'dense_2/kernel:0'), (55, 'dense_2/bias:0')]\n",
      "(3, 3, 3, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 100)\n",
      "(100,)\n",
      "Updated 38 weights. 0 weights were not updated.\n",
      "Updated non-trainable weights: [(28, 'batch_normalization_4/moving_mean:0'), (29, 'batch_normalization_4/moving_variance:0'), (30, 'conv2d_5/kernel:0'), (31, 'conv2d_5/bias:0'), (32, 'batch_normalization_5/gamma:0'), (33, 'batch_normalization_5/beta:0'), (34, 'batch_normalization_5/moving_mean:0'), (35, 'batch_normalization_5/moving_variance:0'), (36, 'conv2d_6/kernel:0'), (37, 'conv2d_6/bias:0'), (38, 'batch_normalization_6/gamma:0'), (39, 'batch_normalization_6/beta:0'), (40, 'batch_normalization_6/moving_mean:0'), (41, 'batch_normalization_6/moving_variance:0'), (42, 'dense/kernel:0'), (43, 'dense/bias:0'), (44, 'batch_normalization_7/gamma:0'), (45, 'batch_normalization_7/beta:0'), (46, 'batch_normalization_7/moving_mean:0'), (47, 'batch_normalization_7/moving_variance:0'), (48, 'dense_1/kernel:0'), (49, 'dense_1/bias:0'), (50, 'batch_normalization_8/gamma:0'), (51, 'batch_normalization_8/beta:0'), (52, 'batch_normalization_8/moving_mean:0'), (53, 'batch_normalization_8/moving_variance:0'), (54, 'dense_2/kernel:0'), (55, 'dense_2/bias:0')]\n",
      "(3, 3, 3, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 100)\n",
      "(100,)\n",
      "Updated 38 weights. 0 weights were not updated.\n",
      "Updated non-trainable weights: [(28, 'batch_normalization_4/moving_mean:0'), (29, 'batch_normalization_4/moving_variance:0'), (30, 'conv2d_5/kernel:0'), (31, 'conv2d_5/bias:0'), (32, 'batch_normalization_5/gamma:0'), (33, 'batch_normalization_5/beta:0'), (34, 'batch_normalization_5/moving_mean:0'), (35, 'batch_normalization_5/moving_variance:0'), (36, 'conv2d_6/kernel:0'), (37, 'conv2d_6/bias:0'), (38, 'batch_normalization_6/gamma:0'), (39, 'batch_normalization_6/beta:0'), (40, 'batch_normalization_6/moving_mean:0'), (41, 'batch_normalization_6/moving_variance:0'), (42, 'dense/kernel:0'), (43, 'dense/bias:0'), (44, 'batch_normalization_7/gamma:0'), (45, 'batch_normalization_7/beta:0'), (46, 'batch_normalization_7/moving_mean:0'), (47, 'batch_normalization_7/moving_variance:0'), (48, 'dense_1/kernel:0'), (49, 'dense_1/bias:0'), (50, 'batch_normalization_8/gamma:0'), (51, 'batch_normalization_8/beta:0'), (52, 'batch_normalization_8/moving_mean:0'), (53, 'batch_normalization_8/moving_variance:0'), (54, 'dense_2/kernel:0'), (55, 'dense_2/bias:0')]\n",
      "(3, 3, 3, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 100)\n",
      "(100,)\n",
      "Updated 38 weights. 0 weights were not updated.\n",
      "Updated non-trainable weights: [(28, 'batch_normalization_4/moving_mean:0'), (29, 'batch_normalization_4/moving_variance:0'), (30, 'conv2d_5/kernel:0'), (31, 'conv2d_5/bias:0'), (32, 'batch_normalization_5/gamma:0'), (33, 'batch_normalization_5/beta:0'), (34, 'batch_normalization_5/moving_mean:0'), (35, 'batch_normalization_5/moving_variance:0'), (36, 'conv2d_6/kernel:0'), (37, 'conv2d_6/bias:0'), (38, 'batch_normalization_6/gamma:0'), (39, 'batch_normalization_6/beta:0'), (40, 'batch_normalization_6/moving_mean:0'), (41, 'batch_normalization_6/moving_variance:0'), (42, 'dense/kernel:0'), (43, 'dense/bias:0'), (44, 'batch_normalization_7/gamma:0'), (45, 'batch_normalization_7/beta:0'), (46, 'batch_normalization_7/moving_mean:0'), (47, 'batch_normalization_7/moving_variance:0'), (48, 'dense_1/kernel:0'), (49, 'dense_1/bias:0'), (50, 'batch_normalization_8/gamma:0'), (51, 'batch_normalization_8/beta:0'), (52, 'batch_normalization_8/moving_mean:0'), (53, 'batch_normalization_8/moving_variance:0'), (54, 'dense_2/kernel:0'), (55, 'dense_2/bias:0')]\n",
      "(3, 3, 3, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 100)\n",
      "(100,)\n",
      "Updated 38 weights. 0 weights were not updated.\n",
      "Updated non-trainable weights: [(28, 'batch_normalization_4/moving_mean:0'), (29, 'batch_normalization_4/moving_variance:0'), (30, 'conv2d_5/kernel:0'), (31, 'conv2d_5/bias:0'), (32, 'batch_normalization_5/gamma:0'), (33, 'batch_normalization_5/beta:0'), (34, 'batch_normalization_5/moving_mean:0'), (35, 'batch_normalization_5/moving_variance:0'), (36, 'conv2d_6/kernel:0'), (37, 'conv2d_6/bias:0'), (38, 'batch_normalization_6/gamma:0'), (39, 'batch_normalization_6/beta:0'), (40, 'batch_normalization_6/moving_mean:0'), (41, 'batch_normalization_6/moving_variance:0'), (42, 'dense/kernel:0'), (43, 'dense/bias:0'), (44, 'batch_normalization_7/gamma:0'), (45, 'batch_normalization_7/beta:0'), (46, 'batch_normalization_7/moving_mean:0'), (47, 'batch_normalization_7/moving_variance:0'), (48, 'dense_1/kernel:0'), (49, 'dense_1/bias:0'), (50, 'batch_normalization_8/gamma:0'), (51, 'batch_normalization_8/beta:0'), (52, 'batch_normalization_8/moving_mean:0'), (53, 'batch_normalization_8/moving_variance:0'), (54, 'dense_2/kernel:0'), (55, 'dense_2/bias:0')]\n",
      "Loading weights from None\n",
      "Saving results to: /home/conite/Documents/WORKSPACE/PROJECTS/Memoire M2/code/MachineUnlearningUpdate/models/poisoning/budget-10000/seed-42/first-order/Cifar100_VGG16_classifier_unlearning_results.json\n",
      "Results saved to: /home/conite/Documents/WORKSPACE/PROJECTS/Memoire M2/code/MachineUnlearningUpdate/models/poisoning/budget-10000/seed-42/first-order/Cifar100_VGG16_classifier_unlearning_results.json\n",
      "* Evaluating VGG16 on Cifar100 after first-order unlearning *\n",
      "Loading weights from None\n",
      "Model weights loaded successfully from /home/conite/Documents/WORKSPACE/PROJECTS/Memoire M2/code/MachineUnlearningUpdate/models/poisoning/budget-10000/seed-42/first-order/Cifar100_VGG16_repaired_model.hdf5.\n",
      "Accuracy Cifar100 model: 0.6082\n",
      "* Second-order unlearning VGG16 on Cifar100 poisoned model *\n",
      "Loading weights from None\n",
      "Early stopping at iteration 113. Update norm 1956731.5948181152 > [1907318.2734069824, 20]\n",
      "(3, 3, 3, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 100)\n",
      "(100,)\n",
      "Updated 38 weights. 0 weights were not updated.\n",
      "Updated non-trainable weights: [(28, 'batch_normalization_4/moving_mean:0'), (29, 'batch_normalization_4/moving_variance:0'), (30, 'conv2d_5/kernel:0'), (31, 'conv2d_5/bias:0'), (32, 'batch_normalization_5/gamma:0'), (33, 'batch_normalization_5/beta:0'), (34, 'batch_normalization_5/moving_mean:0'), (35, 'batch_normalization_5/moving_variance:0'), (36, 'conv2d_6/kernel:0'), (37, 'conv2d_6/bias:0'), (38, 'batch_normalization_6/gamma:0'), (39, 'batch_normalization_6/beta:0'), (40, 'batch_normalization_6/moving_mean:0'), (41, 'batch_normalization_6/moving_variance:0'), (42, 'dense/kernel:0'), (43, 'dense/bias:0'), (44, 'batch_normalization_7/gamma:0'), (45, 'batch_normalization_7/beta:0'), (46, 'batch_normalization_7/moving_mean:0'), (47, 'batch_normalization_7/moving_variance:0'), (48, 'dense_1/kernel:0'), (49, 'dense_1/bias:0'), (50, 'batch_normalization_8/gamma:0'), (51, 'batch_normalization_8/beta:0'), (52, 'batch_normalization_8/moving_mean:0'), (53, 'batch_normalization_8/moving_variance:0'), (54, 'dense_2/kernel:0'), (55, 'dense_2/bias:0')]\n",
      "Early stopping at iteration 20. Update norm 9.92613789556206e+20 > [41930454242.078125, 20]\n",
      "(3, 3, 3, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 100)\n",
      "(100,)\n",
      "Updated 38 weights. 0 weights were not updated.\n",
      "Updated non-trainable weights: [(28, 'batch_normalization_4/moving_mean:0'), (29, 'batch_normalization_4/moving_variance:0'), (30, 'conv2d_5/kernel:0'), (31, 'conv2d_5/bias:0'), (32, 'batch_normalization_5/gamma:0'), (33, 'batch_normalization_5/beta:0'), (34, 'batch_normalization_5/moving_mean:0'), (35, 'batch_normalization_5/moving_variance:0'), (36, 'conv2d_6/kernel:0'), (37, 'conv2d_6/bias:0'), (38, 'batch_normalization_6/gamma:0'), (39, 'batch_normalization_6/beta:0'), (40, 'batch_normalization_6/moving_mean:0'), (41, 'batch_normalization_6/moving_variance:0'), (42, 'dense/kernel:0'), (43, 'dense/bias:0'), (44, 'batch_normalization_7/gamma:0'), (45, 'batch_normalization_7/beta:0'), (46, 'batch_normalization_7/moving_mean:0'), (47, 'batch_normalization_7/moving_variance:0'), (48, 'dense_1/kernel:0'), (49, 'dense_1/bias:0'), (50, 'batch_normalization_8/gamma:0'), (51, 'batch_normalization_8/beta:0'), (52, 'batch_normalization_8/moving_mean:0'), (53, 'batch_normalization_8/moving_variance:0'), (54, 'dense_2/kernel:0'), (55, 'dense_2/bias:0')]\n",
      "(3, 3, 3, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 100)\n",
      "(100,)\n",
      "Updated 38 weights. 0 weights were not updated.\n",
      "Updated non-trainable weights: [(28, 'batch_normalization_4/moving_mean:0'), (29, 'batch_normalization_4/moving_variance:0'), (30, 'conv2d_5/kernel:0'), (31, 'conv2d_5/bias:0'), (32, 'batch_normalization_5/gamma:0'), (33, 'batch_normalization_5/beta:0'), (34, 'batch_normalization_5/moving_mean:0'), (35, 'batch_normalization_5/moving_variance:0'), (36, 'conv2d_6/kernel:0'), (37, 'conv2d_6/bias:0'), (38, 'batch_normalization_6/gamma:0'), (39, 'batch_normalization_6/beta:0'), (40, 'batch_normalization_6/moving_mean:0'), (41, 'batch_normalization_6/moving_variance:0'), (42, 'dense/kernel:0'), (43, 'dense/bias:0'), (44, 'batch_normalization_7/gamma:0'), (45, 'batch_normalization_7/beta:0'), (46, 'batch_normalization_7/moving_mean:0'), (47, 'batch_normalization_7/moving_variance:0'), (48, 'dense_1/kernel:0'), (49, 'dense_1/bias:0'), (50, 'batch_normalization_8/gamma:0'), (51, 'batch_normalization_8/beta:0'), (52, 'batch_normalization_8/moving_mean:0'), (53, 'batch_normalization_8/moving_variance:0'), (54, 'dense_2/kernel:0'), (55, 'dense_2/bias:0')]\n",
      "Early stopping at iteration 20. Update norm 6.245472282067138e+20 > [5882343017.501709, 20]\n",
      "(3, 3, 3, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 100)\n",
      "(100,)\n",
      "Updated 38 weights. 0 weights were not updated.\n",
      "Updated non-trainable weights: [(28, 'batch_normalization_4/moving_mean:0'), (29, 'batch_normalization_4/moving_variance:0'), (30, 'conv2d_5/kernel:0'), (31, 'conv2d_5/bias:0'), (32, 'batch_normalization_5/gamma:0'), (33, 'batch_normalization_5/beta:0'), (34, 'batch_normalization_5/moving_mean:0'), (35, 'batch_normalization_5/moving_variance:0'), (36, 'conv2d_6/kernel:0'), (37, 'conv2d_6/bias:0'), (38, 'batch_normalization_6/gamma:0'), (39, 'batch_normalization_6/beta:0'), (40, 'batch_normalization_6/moving_mean:0'), (41, 'batch_normalization_6/moving_variance:0'), (42, 'dense/kernel:0'), (43, 'dense/bias:0'), (44, 'batch_normalization_7/gamma:0'), (45, 'batch_normalization_7/beta:0'), (46, 'batch_normalization_7/moving_mean:0'), (47, 'batch_normalization_7/moving_variance:0'), (48, 'dense_1/kernel:0'), (49, 'dense_1/bias:0'), (50, 'batch_normalization_8/gamma:0'), (51, 'batch_normalization_8/beta:0'), (52, 'batch_normalization_8/moving_mean:0'), (53, 'batch_normalization_8/moving_variance:0'), (54, 'dense_2/kernel:0'), (55, 'dense_2/bias:0')]\n",
      "Early stopping at iteration 20. Update norm 1.1460099902726136e+22 > [10806612616.909668, 20]\n",
      "(3, 3, 3, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 100)\n",
      "(100,)\n",
      "Updated 38 weights. 0 weights were not updated.\n",
      "Updated non-trainable weights: [(28, 'batch_normalization_4/moving_mean:0'), (29, 'batch_normalization_4/moving_variance:0'), (30, 'conv2d_5/kernel:0'), (31, 'conv2d_5/bias:0'), (32, 'batch_normalization_5/gamma:0'), (33, 'batch_normalization_5/beta:0'), (34, 'batch_normalization_5/moving_mean:0'), (35, 'batch_normalization_5/moving_variance:0'), (36, 'conv2d_6/kernel:0'), (37, 'conv2d_6/bias:0'), (38, 'batch_normalization_6/gamma:0'), (39, 'batch_normalization_6/beta:0'), (40, 'batch_normalization_6/moving_mean:0'), (41, 'batch_normalization_6/moving_variance:0'), (42, 'dense/kernel:0'), (43, 'dense/bias:0'), (44, 'batch_normalization_7/gamma:0'), (45, 'batch_normalization_7/beta:0'), (46, 'batch_normalization_7/moving_mean:0'), (47, 'batch_normalization_7/moving_variance:0'), (48, 'dense_1/kernel:0'), (49, 'dense_1/bias:0'), (50, 'batch_normalization_8/gamma:0'), (51, 'batch_normalization_8/beta:0'), (52, 'batch_normalization_8/moving_mean:0'), (53, 'batch_normalization_8/moving_variance:0'), (54, 'dense_2/kernel:0'), (55, 'dense_2/bias:0')]\n",
      "Early stopping at iteration 20. Update norm 2.0310254918213063e+21 > [5306855704.812744, 20]\n",
      "(3, 3, 3, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 100)\n",
      "(100,)\n",
      "Updated 38 weights. 0 weights were not updated.\n",
      "Updated non-trainable weights: [(28, 'batch_normalization_4/moving_mean:0'), (29, 'batch_normalization_4/moving_variance:0'), (30, 'conv2d_5/kernel:0'), (31, 'conv2d_5/bias:0'), (32, 'batch_normalization_5/gamma:0'), (33, 'batch_normalization_5/beta:0'), (34, 'batch_normalization_5/moving_mean:0'), (35, 'batch_normalization_5/moving_variance:0'), (36, 'conv2d_6/kernel:0'), (37, 'conv2d_6/bias:0'), (38, 'batch_normalization_6/gamma:0'), (39, 'batch_normalization_6/beta:0'), (40, 'batch_normalization_6/moving_mean:0'), (41, 'batch_normalization_6/moving_variance:0'), (42, 'dense/kernel:0'), (43, 'dense/bias:0'), (44, 'batch_normalization_7/gamma:0'), (45, 'batch_normalization_7/beta:0'), (46, 'batch_normalization_7/moving_mean:0'), (47, 'batch_normalization_7/moving_variance:0'), (48, 'dense_1/kernel:0'), (49, 'dense_1/bias:0'), (50, 'batch_normalization_8/gamma:0'), (51, 'batch_normalization_8/beta:0'), (52, 'batch_normalization_8/moving_mean:0'), (53, 'batch_normalization_8/moving_variance:0'), (54, 'dense_2/kernel:0'), (55, 'dense_2/bias:0')]\n",
      "(3, 3, 3, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 100)\n",
      "(100,)\n",
      "Updated 38 weights. 0 weights were not updated.\n",
      "Updated non-trainable weights: [(28, 'batch_normalization_4/moving_mean:0'), (29, 'batch_normalization_4/moving_variance:0'), (30, 'conv2d_5/kernel:0'), (31, 'conv2d_5/bias:0'), (32, 'batch_normalization_5/gamma:0'), (33, 'batch_normalization_5/beta:0'), (34, 'batch_normalization_5/moving_mean:0'), (35, 'batch_normalization_5/moving_variance:0'), (36, 'conv2d_6/kernel:0'), (37, 'conv2d_6/bias:0'), (38, 'batch_normalization_6/gamma:0'), (39, 'batch_normalization_6/beta:0'), (40, 'batch_normalization_6/moving_mean:0'), (41, 'batch_normalization_6/moving_variance:0'), (42, 'dense/kernel:0'), (43, 'dense/bias:0'), (44, 'batch_normalization_7/gamma:0'), (45, 'batch_normalization_7/beta:0'), (46, 'batch_normalization_7/moving_mean:0'), (47, 'batch_normalization_7/moving_variance:0'), (48, 'dense_1/kernel:0'), (49, 'dense_1/bias:0'), (50, 'batch_normalization_8/gamma:0'), (51, 'batch_normalization_8/beta:0'), (52, 'batch_normalization_8/moving_mean:0'), (53, 'batch_normalization_8/moving_variance:0'), (54, 'dense_2/kernel:0'), (55, 'dense_2/bias:0')]\n",
      "(3, 3, 3, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 100)\n",
      "(100,)\n",
      "Updated 38 weights. 0 weights were not updated.\n",
      "Updated non-trainable weights: [(28, 'batch_normalization_4/moving_mean:0'), (29, 'batch_normalization_4/moving_variance:0'), (30, 'conv2d_5/kernel:0'), (31, 'conv2d_5/bias:0'), (32, 'batch_normalization_5/gamma:0'), (33, 'batch_normalization_5/beta:0'), (34, 'batch_normalization_5/moving_mean:0'), (35, 'batch_normalization_5/moving_variance:0'), (36, 'conv2d_6/kernel:0'), (37, 'conv2d_6/bias:0'), (38, 'batch_normalization_6/gamma:0'), (39, 'batch_normalization_6/beta:0'), (40, 'batch_normalization_6/moving_mean:0'), (41, 'batch_normalization_6/moving_variance:0'), (42, 'dense/kernel:0'), (43, 'dense/bias:0'), (44, 'batch_normalization_7/gamma:0'), (45, 'batch_normalization_7/beta:0'), (46, 'batch_normalization_7/moving_mean:0'), (47, 'batch_normalization_7/moving_variance:0'), (48, 'dense_1/kernel:0'), (49, 'dense_1/bias:0'), (50, 'batch_normalization_8/gamma:0'), (51, 'batch_normalization_8/beta:0'), (52, 'batch_normalization_8/moving_mean:0'), (53, 'batch_normalization_8/moving_variance:0'), (54, 'dense_2/kernel:0'), (55, 'dense_2/bias:0')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Applications/Poisoning/unlearn/common.py:210: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig, ax = plt.subplots(figsize=(10, 10))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at iteration 20. Update norm 1.675372601765993e+21 > [26590300868.25586, 20]\n",
      "(3, 3, 3, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 100)\n",
      "(100,)\n",
      "Updated 38 weights. 0 weights were not updated.\n",
      "Updated non-trainable weights: [(28, 'batch_normalization_4/moving_mean:0'), (29, 'batch_normalization_4/moving_variance:0'), (30, 'conv2d_5/kernel:0'), (31, 'conv2d_5/bias:0'), (32, 'batch_normalization_5/gamma:0'), (33, 'batch_normalization_5/beta:0'), (34, 'batch_normalization_5/moving_mean:0'), (35, 'batch_normalization_5/moving_variance:0'), (36, 'conv2d_6/kernel:0'), (37, 'conv2d_6/bias:0'), (38, 'batch_normalization_6/gamma:0'), (39, 'batch_normalization_6/beta:0'), (40, 'batch_normalization_6/moving_mean:0'), (41, 'batch_normalization_6/moving_variance:0'), (42, 'dense/kernel:0'), (43, 'dense/bias:0'), (44, 'batch_normalization_7/gamma:0'), (45, 'batch_normalization_7/beta:0'), (46, 'batch_normalization_7/moving_mean:0'), (47, 'batch_normalization_7/moving_variance:0'), (48, 'dense_1/kernel:0'), (49, 'dense_1/bias:0'), (50, 'batch_normalization_8/gamma:0'), (51, 'batch_normalization_8/beta:0'), (52, 'batch_normalization_8/moving_mean:0'), (53, 'batch_normalization_8/moving_variance:0'), (54, 'dense_2/kernel:0'), (55, 'dense_2/bias:0')]\n",
      "Early stopping at iteration 20. Update norm 2.97025338369375e+21 > [1418867848.7670898, 20]\n",
      "(3, 3, 3, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 100)\n",
      "(100,)\n",
      "Updated 38 weights. 0 weights were not updated.\n",
      "Updated non-trainable weights: [(28, 'batch_normalization_4/moving_mean:0'), (29, 'batch_normalization_4/moving_variance:0'), (30, 'conv2d_5/kernel:0'), (31, 'conv2d_5/bias:0'), (32, 'batch_normalization_5/gamma:0'), (33, 'batch_normalization_5/beta:0'), (34, 'batch_normalization_5/moving_mean:0'), (35, 'batch_normalization_5/moving_variance:0'), (36, 'conv2d_6/kernel:0'), (37, 'conv2d_6/bias:0'), (38, 'batch_normalization_6/gamma:0'), (39, 'batch_normalization_6/beta:0'), (40, 'batch_normalization_6/moving_mean:0'), (41, 'batch_normalization_6/moving_variance:0'), (42, 'dense/kernel:0'), (43, 'dense/bias:0'), (44, 'batch_normalization_7/gamma:0'), (45, 'batch_normalization_7/beta:0'), (46, 'batch_normalization_7/moving_mean:0'), (47, 'batch_normalization_7/moving_variance:0'), (48, 'dense_1/kernel:0'), (49, 'dense_1/bias:0'), (50, 'batch_normalization_8/gamma:0'), (51, 'batch_normalization_8/beta:0'), (52, 'batch_normalization_8/moving_mean:0'), (53, 'batch_normalization_8/moving_variance:0'), (54, 'dense_2/kernel:0'), (55, 'dense_2/bias:0')]\n",
      "Loading weights from None\n",
      "Saving results to: /home/conite/Documents/WORKSPACE/PROJECTS/Memoire M2/code/MachineUnlearningUpdate/models/poisoning/budget-10000/seed-42/second-order/Cifar100_VGG16_classifier_unlearning_results.json\n",
      "Results saved to: /home/conite/Documents/WORKSPACE/PROJECTS/Memoire M2/code/MachineUnlearningUpdate/models/poisoning/budget-10000/seed-42/second-order/Cifar100_VGG16_classifier_unlearning_results.json\n",
      "* Evaluating VGG16 on Cifar100 after second-order unlearning *\n",
      "Loading weights from None\n",
      "Model weights loaded successfully from /home/conite/Documents/WORKSPACE/PROJECTS/Memoire M2/code/MachineUnlearningUpdate/models/poisoning/budget-10000/seed-42/second-order/Cifar100_VGG16_repaired_model.hdf5.\n",
      "Accuracy Cifar100 model: 0.0058\n",
      "* Evaluating VGG16 on Cifar100 poisoned model *\n",
      "Loading weights from None\n",
      "Model weights loaded successfully from /home/conite/Documents/WORKSPACE/PROJECTS/Memoire M2/code/MachineUnlearningUpdate/models/poisoning/budget-10000/seed-42/Cifar100_VGG16_poisoned_model.hdf5.\n",
      "Accuracy Cifar100 model: 0.6082\n",
      "* First-order unlearning VGG16 on Cifar100 poisoned model *\n",
      "Loading weights from None\n",
      "(3, 3, 3, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 100)\n",
      "(100,)\n",
      "Updated 38 weights. 0 weights were not updated.\n",
      "Updated non-trainable weights: [(28, 'batch_normalization_4/moving_mean:0'), (29, 'batch_normalization_4/moving_variance:0'), (30, 'conv2d_5/kernel:0'), (31, 'conv2d_5/bias:0'), (32, 'batch_normalization_5/gamma:0'), (33, 'batch_normalization_5/beta:0'), (34, 'batch_normalization_5/moving_mean:0'), (35, 'batch_normalization_5/moving_variance:0'), (36, 'conv2d_6/kernel:0'), (37, 'conv2d_6/bias:0'), (38, 'batch_normalization_6/gamma:0'), (39, 'batch_normalization_6/beta:0'), (40, 'batch_normalization_6/moving_mean:0'), (41, 'batch_normalization_6/moving_variance:0'), (42, 'dense/kernel:0'), (43, 'dense/bias:0'), (44, 'batch_normalization_7/gamma:0'), (45, 'batch_normalization_7/beta:0'), (46, 'batch_normalization_7/moving_mean:0'), (47, 'batch_normalization_7/moving_variance:0'), (48, 'dense_1/kernel:0'), (49, 'dense_1/bias:0'), (50, 'batch_normalization_8/gamma:0'), (51, 'batch_normalization_8/beta:0'), (52, 'batch_normalization_8/moving_mean:0'), (53, 'batch_normalization_8/moving_variance:0'), (54, 'dense_2/kernel:0'), (55, 'dense_2/bias:0')]\n",
      "(3, 3, 3, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 100)\n",
      "(100,)\n",
      "Updated 38 weights. 0 weights were not updated.\n",
      "Updated non-trainable weights: [(28, 'batch_normalization_4/moving_mean:0'), (29, 'batch_normalization_4/moving_variance:0'), (30, 'conv2d_5/kernel:0'), (31, 'conv2d_5/bias:0'), (32, 'batch_normalization_5/gamma:0'), (33, 'batch_normalization_5/beta:0'), (34, 'batch_normalization_5/moving_mean:0'), (35, 'batch_normalization_5/moving_variance:0'), (36, 'conv2d_6/kernel:0'), (37, 'conv2d_6/bias:0'), (38, 'batch_normalization_6/gamma:0'), (39, 'batch_normalization_6/beta:0'), (40, 'batch_normalization_6/moving_mean:0'), (41, 'batch_normalization_6/moving_variance:0'), (42, 'dense/kernel:0'), (43, 'dense/bias:0'), (44, 'batch_normalization_7/gamma:0'), (45, 'batch_normalization_7/beta:0'), (46, 'batch_normalization_7/moving_mean:0'), (47, 'batch_normalization_7/moving_variance:0'), (48, 'dense_1/kernel:0'), (49, 'dense_1/bias:0'), (50, 'batch_normalization_8/gamma:0'), (51, 'batch_normalization_8/beta:0'), (52, 'batch_normalization_8/moving_mean:0'), (53, 'batch_normalization_8/moving_variance:0'), (54, 'dense_2/kernel:0'), (55, 'dense_2/bias:0')]\n",
      "(3, 3, 3, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 100)\n",
      "(100,)\n",
      "Updated 38 weights. 0 weights were not updated.\n",
      "Updated non-trainable weights: [(28, 'batch_normalization_4/moving_mean:0'), (29, 'batch_normalization_4/moving_variance:0'), (30, 'conv2d_5/kernel:0'), (31, 'conv2d_5/bias:0'), (32, 'batch_normalization_5/gamma:0'), (33, 'batch_normalization_5/beta:0'), (34, 'batch_normalization_5/moving_mean:0'), (35, 'batch_normalization_5/moving_variance:0'), (36, 'conv2d_6/kernel:0'), (37, 'conv2d_6/bias:0'), (38, 'batch_normalization_6/gamma:0'), (39, 'batch_normalization_6/beta:0'), (40, 'batch_normalization_6/moving_mean:0'), (41, 'batch_normalization_6/moving_variance:0'), (42, 'dense/kernel:0'), (43, 'dense/bias:0'), (44, 'batch_normalization_7/gamma:0'), (45, 'batch_normalization_7/beta:0'), (46, 'batch_normalization_7/moving_mean:0'), (47, 'batch_normalization_7/moving_variance:0'), (48, 'dense_1/kernel:0'), (49, 'dense_1/bias:0'), (50, 'batch_normalization_8/gamma:0'), (51, 'batch_normalization_8/beta:0'), (52, 'batch_normalization_8/moving_mean:0'), (53, 'batch_normalization_8/moving_variance:0'), (54, 'dense_2/kernel:0'), (55, 'dense_2/bias:0')]\n",
      "(3, 3, 3, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 100)\n",
      "(100,)\n",
      "Updated 38 weights. 0 weights were not updated.\n",
      "Updated non-trainable weights: [(28, 'batch_normalization_4/moving_mean:0'), (29, 'batch_normalization_4/moving_variance:0'), (30, 'conv2d_5/kernel:0'), (31, 'conv2d_5/bias:0'), (32, 'batch_normalization_5/gamma:0'), (33, 'batch_normalization_5/beta:0'), (34, 'batch_normalization_5/moving_mean:0'), (35, 'batch_normalization_5/moving_variance:0'), (36, 'conv2d_6/kernel:0'), (37, 'conv2d_6/bias:0'), (38, 'batch_normalization_6/gamma:0'), (39, 'batch_normalization_6/beta:0'), (40, 'batch_normalization_6/moving_mean:0'), (41, 'batch_normalization_6/moving_variance:0'), (42, 'dense/kernel:0'), (43, 'dense/bias:0'), (44, 'batch_normalization_7/gamma:0'), (45, 'batch_normalization_7/beta:0'), (46, 'batch_normalization_7/moving_mean:0'), (47, 'batch_normalization_7/moving_variance:0'), (48, 'dense_1/kernel:0'), (49, 'dense_1/bias:0'), (50, 'batch_normalization_8/gamma:0'), (51, 'batch_normalization_8/beta:0'), (52, 'batch_normalization_8/moving_mean:0'), (53, 'batch_normalization_8/moving_variance:0'), (54, 'dense_2/kernel:0'), (55, 'dense_2/bias:0')]\n",
      "(3, 3, 3, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 64)\n",
      "(64,)\n",
      "(64,)\n",
      "(64,)\n",
      "(3, 3, 64, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(3, 3, 128, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(3, 3, 256, 256)\n",
      "(256,)\n",
      "(256,)\n",
      "(256,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 4096)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096,)\n",
      "(4096, 100)\n",
      "(100,)\n",
      "Updated 38 weights. 0 weights were not updated.\n",
      "Updated non-trainable weights: [(28, 'batch_normalization_4/moving_mean:0'), (29, 'batch_normalization_4/moving_variance:0'), (30, 'conv2d_5/kernel:0'), (31, 'conv2d_5/bias:0'), (32, 'batch_normalization_5/gamma:0'), (33, 'batch_normalization_5/beta:0'), (34, 'batch_normalization_5/moving_mean:0'), (35, 'batch_normalization_5/moving_variance:0'), (36, 'conv2d_6/kernel:0'), (37, 'conv2d_6/bias:0'), (38, 'batch_normalization_6/gamma:0'), (39, 'batch_normalization_6/beta:0'), (40, 'batch_normalization_6/moving_mean:0'), (41, 'batch_normalization_6/moving_variance:0'), (42, 'dense/kernel:0'), (43, 'dense/bias:0'), (44, 'batch_normalization_7/gamma:0'), (45, 'batch_normalization_7/beta:0'), (46, 'batch_normalization_7/moving_mean:0'), (47, 'batch_normalization_7/moving_variance:0'), (48, 'dense_1/kernel:0'), (49, 'dense_1/bias:0'), (50, 'batch_normalization_8/gamma:0'), (51, 'batch_normalization_8/beta:0'), (52, 'batch_normalization_8/moving_mean:0'), (53, 'batch_normalization_8/moving_variance:0'), (54, 'dense_2/kernel:0'), (55, 'dense_2/bias:0')]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "results = {\n",
    "    'clean': {},\n",
    "    'poisoned': {},\n",
    "    'first_order_unlearning': {},\n",
    "    'second_order_unlearning': {}\n",
    "}\n",
    "\n",
    "update_targets = ['classifier', 'feature_extractor']\n",
    "\n",
    "for dataset in datasets:\n",
    "    results['clean'][dataset] = {}\n",
    "    results['poisoned'][dataset] = {}\n",
    "    results['first_order_unlearning'][dataset] = {}\n",
    "    results['second_order_unlearning'][dataset] = {}\n",
    "   \n",
    "    print('#' * 60)\n",
    "    print(f\" UNLEARNING \")\n",
    "    print('#' * 60)\n",
    "    print('\\n\\n')\n",
    "\n",
    "    for modelname in modelnames:\n",
    "        for update_target in update_targets:\n",
    "            print(f\"* Evaluating {modelname} on {dataset} poisoned model *\")\n",
    "            poisoned_accuracy = evaluate(model_folder=poisoned_folder, dataset=dataset, modelname=modelname, type='poisoned')\n",
    "            results['poisoned'][dataset][modelname] = poisoned_accuracy\n",
    "            \n",
    "            print(f\"* First-order unlearning {modelname} on {dataset} poisoned model *\")\n",
    "            fo_experiment(poisoned_folder/'first-order', train_kwargs, poison_kwargs, fo_unlearn_kwargs, dataset=dataset, modelname=modelname, update_target=update_target)\n",
    "            print(f\"* Evaluating {modelname} on {dataset} after first-order unlearning *\")\n",
    "            fo_repaired_accuracy = evaluate(model_folder=first_unlearn_folder, dataset=dataset, modelname=modelname, type='repaired')\n",
    "            results['first_order_unlearning'][dataset][modelname] = fo_repaired_accuracy\n",
    "        \n",
    "\n",
    "            print(f\"* Second-order unlearning {modelname} on {dataset} poisoned model *\")\n",
    "            so_experiment(poisoned_folder/'second-order', train_kwargs, poison_kwargs, so_unlearn_kwargs, dataset=dataset, modelname=modelname, update_target=update_target)\n",
    "            print(f\"* Evaluating {modelname} on {dataset} after second-order unlearning *\")\n",
    "            so_repaired_accuracy = evaluate(model_folder=second_unlearn_folder, dataset=dataset, modelname=modelname, type='repaired')\n",
    "            results['second_order_unlearning'][dataset][modelname] = so_repaired_accuracy\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FORGET SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from torch.utils.data import DataLoader\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def calculate_confidence(model, x_data):\n",
    "    y_pred = model.predict(x_data)\n",
    "    probs = tf.nn.softmax(y_pred, axis=1).numpy()\n",
    "    max_probs = np.max(probs, axis=1)\n",
    "    return max_probs\n",
    "\n",
    "\n",
    "def calculate_confusion_matrix(model, x_data, y_true):\n",
    "    y_pred = model.predict(x_data)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = np.argmax(y_true, axis=1) if len(y_true.shape) > 1 else y_true\n",
    "    cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "    accuracy = np.trace(cm) / np.sum(cm)\n",
    "    if cm.shape == (2, 2):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "    else:\n",
    "        tn, fp, fn, tp = 0, 0, 0, 0\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                if i == j:\n",
    "                    if i == 1:\n",
    "                        tp = cm[i, j]\n",
    "                    else:\n",
    "                        tn += cm[i, j]\n",
    "                else:\n",
    "                    if i == 1:\n",
    "                        fn += cm[i, j]\n",
    "                    else:\n",
    "                        fp += cm[i, j]\n",
    "    return tn, fp, fn, tp\n",
    "\n",
    "def calculate_forget_score(tn_before, fp_before, fn_before, tp_before, tn_after, fp_after, fn_after, tp_after):\n",
    "    delta = 0.01\n",
    "    tpr_before = tp_before / (tp_before + fn_before + delta)\n",
    "    fpr_before = fp_before / (fp_before + tn_before + delta)\n",
    "    tpr_after = tp_after / (tp_after + fn_after + delta)\n",
    "    fpr_after = fp_after / (fp_after + tn_after + delta)\n",
    "\n",
    "    epsilon = np.nanmax([\n",
    "        np.log(1 - delta - fpr_after) - np.log(tpr_after),\n",
    "        np.log(1 - delta - fn_after) - np.log(tpr_after),\n",
    "        np.log(1 - delta - fpr_before) - np.log(tpr_before),\n",
    "        np.log(1 - delta - fn_before) - np.log(tpr_before)\n",
    "    ])\n",
    "\n",
    "    return epsilon\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title):\n",
    "    if len(y_true.shape) > 1:\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluate_model_accuracy(model, x_test, y_test):\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = np.argmax(y_test, axis=1) if len(y_test.shape) > 1 else y_test\n",
    "    accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
    "    return accuracy\n",
    "\n",
    "def load_and_evaluate_models(datasets, models, clean_folder, unlearn_folder):\n",
    "    results = {}\n",
    "    \n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        results[dataset_name] = {}\n",
    "        \n",
    "        # Load the dataset\n",
    "        (x_train, y_train), (x_test, y_test), (x_valid, y_valid) = dataset.load()\n",
    "        \n",
    "        for model_name in models[dataset_name]:\n",
    "            model_fn = models[dataset_name][model_name]\n",
    "            results[dataset_name][model_name] = {}\n",
    "            print(f\"Evaluating {model_name} on {dataset_name}\")\n",
    "            \n",
    "            try:\n",
    "                # Load the clean model\n",
    "                model_clean = model_fn()\n",
    "                model_clean.load_weights(clean_folder / f'{model_name}_best_model.hdf5')\n",
    "\n",
    "                # Load the unlearned model\n",
    "                model_unlearned = model_fn()\n",
    "                model_unlearned.load_weights(unlearn_folder / f'{model_name}_repaired_model.hdf5')\n",
    "            except:\n",
    "                print(f\"Error loading models for {model_name} on {dataset_name}\")\n",
    "                continue\n",
    "            # Evaluate the accuracy of the models\n",
    "            accuracy_clean = evaluate_model_accuracy(model_clean, x_test, y_test)\n",
    "            accuracy_unlearned = evaluate_model_accuracy(model_unlearned, x_test, y_test)\n",
    "            print(f\"Accuracy of the clean model: {accuracy_clean:.4f}\")\n",
    "            print(f\"Accuracy of the unlearned model: {accuracy_unlearned:.4f}\")\n",
    "\n",
    "            # Compute confusion matrix for clean model\n",
    "            y_pred_clean = model_clean.predict(x_test).argmax(axis=1)\n",
    "            #plot_confusion_matrix(y_test, y_pred_clean, f'{model_name} Clean Model Confusion Matrix')\n",
    "\n",
    "            # Compute confusion matrix for unlearned model\n",
    "            y_pred_unlearned = model_unlearned.predict(x_test).argmax(axis=1)\n",
    "            #plot_confusion_matrix(y_test, y_pred_unlearned, f'{model_name} Unlearned Model Confusion Matrix')\n",
    "\n",
    "\n",
    "            # Compute confidence and confusion matrix for clean model\n",
    "            clean_confidences = calculate_confidence(model_clean, x_test)\n",
    "            tn_clean, fp_clean, fn_clean, tp_clean = calculate_confusion_matrix(model_clean, x_test, y_test)\n",
    "\n",
    "            # Compute confidence and confusion matrix for unlearned model\n",
    "            unlearning_confidences = calculate_confidence(model_unlearned, x_test)\n",
    "            tn_unlearned, fp_unlearned, fn_unlearned, tp_unlearned = calculate_confusion_matrix(model_unlearned, x_test, y_test)\n",
    "\n",
    "            # Calculate forget score\n",
    "            forget_score = calculate_forget_score(tn_clean, fp_clean, fn_clean, tp_clean, tn_unlearned, fp_unlearned, fn_unlearned, tp_unlearned)\n",
    "            print(f\"Forget Score for {model_name} on {dataset_name}: {forget_score:.4f}\")\n",
    "\n",
    "            results[dataset_name][model_name] = {\n",
    "                'clean_accuracy': accuracy_clean,\n",
    "                'unlearned_accuracy': accuracy_unlearned,\n",
    "                'forget_score': forget_score\n",
    "            }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Cifar10_VGG16 on Cifar10\n",
      "Loading weights from None\n",
      "Error loading models for Cifar10_VGG16 on Cifar10\n",
      "Evaluating Cifar10_RESNET50 on Cifar10\n",
      "Loading weights from None\n",
      "Error loading models for Cifar10_RESNET50 on Cifar10\n",
      "Evaluating SVHN_VGG16 on SVHN\n",
      "Loading weights from None\n",
      "Error loading models for SVHN_VGG16 on SVHN\n",
      "Evaluating SVHN_RESNET50 on SVHN\n",
      "Loading weights from None\n",
      "Error loading models for SVHN_RESNET50 on SVHN\n",
      "Evaluating Cifar100_VGG16 on Cifar100\n",
      "Loading weights from None\n",
      "Error loading models for Cifar100_VGG16 on Cifar100\n",
      "Evaluating Cifar100_RESNET50 on Cifar100\n",
      "Loading weights from None\n",
      "Error loading models for Cifar100_RESNET50 on Cifar100\n",
      "Cifar10 - Cifar10_VGG16:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'clean_accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17373/3072833668.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_results\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{dataset_name} - {model_name}:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  Clean Accuracy: {model_results['clean_accuracy']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  Unlearned Accuracy: {model_results['unlearned_accuracy']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  Forget Score: {model_results['forget_score']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'clean_accuracy'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "# import TensorDataset\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "from conf import BASE_DIR\n",
    "from Applications.Poisoning.gen_configs import main as gen_configs\n",
    "from Applications.Poisoning.model import extractfeatures_VGG16, classifier_VGG16, extractfeatures_RESNET50, classifier_RESNET50, get_VGG16_CIFAR100, get_VGG16_CIFAR10, get_VGG16_SVHN, get_RESNET50_CIFAR100, get_RESNET50_CIFAR10, get_RESNET50_SVHN, extractfeatures_RESNET50_CIFAR100, extractfeatures_VGG16_CIFAR100, classifier_RESNET50_CIFAR100, classifier_VGG16_CIFAR100\n",
    "from Applications.Poisoning.dataset import Cifar10, SVHN, FashionMnist, Cifar100\n",
    "\n",
    "\n",
    "model_folder = BASE_DIR/'models'/'poisoning'\n",
    "\n",
    "datasets = {\n",
    "    'Cifar10': Cifar10,\n",
    "    'SVHN': SVHN,\n",
    "    'Cifar100': Cifar100\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'Cifar10': {\n",
    "        'Cifar10_VGG16': get_VGG16_CIFAR10,\n",
    "        'Cifar10_RESNET50': get_RESNET50_CIFAR10,\n",
    "    },\n",
    "    'SVHN': {\n",
    "        'SVHN_VGG16': get_VGG16_SVHN,\n",
    "        'SVHN_RESNET50': get_RESNET50_SVHN,\n",
    "    },\n",
    "    'Cifar100': {\n",
    "        'Cifar100_VGG16': get_VGG16_CIFAR100,\n",
    "        'Cifar100_RESNET50': get_RESNET50_CIFAR100,\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "clean_folder = model_folder/'clean'\n",
    "poisoned_folder = model_folder/'budget-10000'/'seed-42'\n",
    "first_unlearn_folder = model_folder/'budget-10000'/'seed-42'/'first-order'\n",
    "second_unlearn_folder = model_folder/'budget-10000'/'seed-42'/'second-order'\n",
    "\n",
    "# Run evaluation with load_and_evaluate_models\n",
    "results = load_and_evaluate_models(datasets, models, clean_folder, first_unlearn_folder)\n",
    "\n",
    "# Print final results\n",
    "for dataset_name, dataset_results in results.items():\n",
    "    for model_name, model_results in dataset_results.items():\n",
    "        print(f\"{dataset_name} - {model_name}:\")\n",
    "        print(f\"  Clean Accuracy: {model_results['clean_accuracy']:.4f}\")\n",
    "        print(f\"  Unlearned Accuracy: {model_results['unlearned_accuracy']:.4f}\")\n",
    "        print(f\"  Forget Score: {model_results['forget_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import os\n",
    "\n",
    "# results = {\n",
    "#     'clean': {},\n",
    "#     'poisoned': {},\n",
    "#     'first_order_unlearning': {},\n",
    "#     'second_order_unlearning': {}\n",
    "# }\n",
    "\n",
    "# update_targets = ['feature_extractor', 'classifier']\n",
    "# for dataset in datasets:\n",
    "#     for modelname in modelnames:\n",
    "#         for update_target in update_targets:\n",
    "#             print(f\"* First-order unlearning {modelname} on {dataset} poisoned model *\")\n",
    "#             fo_experiment(poisoned_folder/'first-order', train_kwargs, poison_kwargs, fo_unlearn_kwargs, dataset=dataset, modelname=modelname, update_target=update_target)\n",
    "#             print(f\" * Second-order unlearning {modelname} on {dataset} poisoned model *\")  \n",
    "#             so_experiment(poisoned_folder/'second-order', train_kwargs, poison_kwargs, so_unlearn_kwargs, dataset=dataset, modelname=modelname, update_target=update_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
