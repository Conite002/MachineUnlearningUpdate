{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Build and Train the Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-11 18:05:22.904767: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-12.2/lib64\n",
      "2024-07-11 18:05:22.904783: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Embedding, LSTM, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger,  EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skewnorm\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "import  scipy.sparse as sp\n",
    "from Applications.CanaryRemoval.CanaryRemoval import unlearn_canary, get_z_delta\n",
    "from Unlearner.RNNUnlearner import RNNUNlearner\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch < 5:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return 0.0001\n",
    "\n",
    "\n",
    "class TextModel:\n",
    "    def __init__(self, filename, seq_length, canary, canary_insertions, vocab_size, embedding_dim, lstm_units, batch_size, dropout_rate=0.2, canary_start=None):\n",
    "        self.filename = filename\n",
    "        self.seq_length = seq_length\n",
    "        self.canary = canary\n",
    "        self.canary_insertions = canary_insertions\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lstm_units = lstm_units\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.int2char = {}\n",
    "        self.char2int = {}\n",
    "        self.canary_number = None\n",
    "        if canary_start is not None:\n",
    "            self.canary_start = 'my telephone number is '\n",
    "        self.model = self.build_model()\n",
    "        self.X, self.y, self.int2char = self.load_data()\n",
    "\n",
    "    def set_char2int(self, int2char):\n",
    "        self.char2int = {v: k for k, v in int2char.items()}\n",
    "    \n",
    "    def set_canary_start(self, canary_start):\n",
    "        self.canary_start = canary_start\n",
    "\n",
    "    def load_data(self):\n",
    "        np.random.seed(42)\n",
    "        raw_text = open(self.filename, 'r', encoding='utf-8').read()[265:]  # Charger le texte brut (en supposant que le début est ignoré)\n",
    "        raw_text = self.insert_canary(raw_text)  # Insérer le canary dans le texte brut\n",
    "        raw_text = raw_text.lower()  # Convertir en minuscules\n",
    "        chars = sorted(list(set(raw_text)))  # Obtenir tous les caractères uniques dans le texte\n",
    "\n",
    "        print(\"unique characters : \", chars)\n",
    "        print(\"Number of unique characters: \", len(chars))\n",
    "\n",
    "        # Initialize char2int and int2char using the unique characters\n",
    "        for i, c in enumerate(chars):\n",
    "            self.char2int[c] = i\n",
    "            self.int2char[i] = c\n",
    "            \n",
    "        self.int2char = {i: c for i, c in enumerate(chars)}\n",
    "        self.set_char2int(self.int2char)\n",
    "            \n",
    "        n_chars = len(raw_text)\n",
    "        dataX = []\n",
    "        dataY = []\n",
    "        # Générer des paires d'entrée-sortie codées en entiers                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
    "        for i in range(0, n_chars - self.seq_length, 1):\n",
    "            seq_in = raw_text[i:i + self.seq_length]\n",
    "            seq_out = raw_text[i + self.seq_length]                                                                                                                                                                                                                                                                            \n",
    "            dataX.append([self.char2int[char] for char in seq_in])\n",
    "            dataY.append(self.char2int[seq_out])\n",
    "        n_patterns = len(dataX)\n",
    "        X = np.reshape(dataX, (n_patterns, self.seq_length, 1))\n",
    "        y = to_categorical(dataY)\n",
    "        return X, y, self.int2char\n",
    "\n",
    "    def insert_canary(self, text):\n",
    "        if self.canary_insertions == 0:\n",
    "            return text\n",
    "        canary_len = len(self.canary)  # Longueur du canary\n",
    "        breaks = [m.start() for m in re.finditer('\\n\\n  ', text)]\n",
    "        insertion_points = sorted(np.random.choice(breaks, self.canary_insertions, replace=False))\n",
    "        new_text = ''\n",
    "        for idx in range(len(insertion_points)):\n",
    "            point_pre = insertion_points[idx - 1] + canary_len if idx != 0 else 0\n",
    "            point_last = insertion_points[idx] + canary_len\n",
    "            new_text += text[point_pre:point_last] + self.canary\n",
    "        new_text += text[point_last:]\n",
    "        return new_text\n",
    "    \n",
    "    def find_insertion_points(self, text):\n",
    "        \"\"\"Find appropriate points to insert the canary string.\"\"\"\n",
    "        canary_len = len(self.canary)\n",
    "        breakpoints = [m.start() for m in re.finditer(r'\\s+', text)]\n",
    "        if len(breakpoints) < self.canary_insertions:\n",
    "            raise ValueError(\"Not enough breakpoints to insert the canary string the specified number of times.\")\n",
    "        insertion_points = sorted(random.sample(breakpoints, self.canary_insertions))\n",
    "        return insertion_points\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim))\n",
    "        model.add(LSTM(self.lstm_units, return_sequences=True))\n",
    "        # model.add(Dropout(self.dropout_rate))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(LSTM(self.lstm_units, return_sequences=True))\n",
    "        # model.add(Dropout(self.dropout_rate))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(LSTM(self.lstm_units))\n",
    "        model.add(Dropout(self.dropout_rate))\n",
    "        model.add(Dense(self.vocab_size, activation='softmax'))\n",
    "        optimizer = Adam(learning_rate=0.001)\n",
    "        model.compile(loss=categorical_crossentropy, optimizer=optimizer, metrics=['accuracy'])\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def train(self, x_train, y_train, epochs, model_folder):\n",
    "        customer_folder = os.path.join(model_folder)\n",
    "        if not os.path.join(customer_folder):\n",
    "            os.makedirs(customer_folder)\n",
    "        \n",
    "           # Check if the model is already trained\n",
    "        if os.path.exists(os.path.join(customer_folder, 'final_model.h5')):\n",
    "            print(\"Model already trained. Loading...\")\n",
    "            self.model = load_model(os.path.join(customer_folder, 'final_model.h5'))\n",
    "            return\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "        \n",
    "        checkpoint_path = os.path.join(customer_folder, 'model_checkpoint.ckpt')\n",
    "        csv_logger = CSVLogger(os.path.join(customer_folder, 'training.log'))\n",
    "        checkpoint = ModelCheckpoint(checkpoint_path, save_best_only=True, verbose=1)\n",
    "        callbacks = [csv_logger, early_stopping, LearningRateScheduler(lr_schedule, verbose=1), checkpoint]\n",
    "        self.model.fit(x_train, y_train, batch_size=self.batch_size, epochs=epochs, callbacks=callbacks)\n",
    "        self.model.save(os.path.join(customer_folder, 'final_model.h5'))\n",
    "\n",
    "    def generate_text(self, start_string, num_generate=25, temperature=1.0):\n",
    "        if start_string is None:\n",
    "            start = np.random.randint(0, len(self.X) - 1)\n",
    "            pattern = self.X[start].squeeze()\n",
    "        else:\n",
    "            pattern = np.array([self.char2int[char] for char in start_string])\n",
    "\n",
    "        print(\"Seed:\")\n",
    "        print(\"\\\"\", ''.join([self.int2char[value] for value in pattern]), \"\\\"\")\n",
    "        print(\"Generation of text: \\n\")\n",
    "\n",
    "        text_generated = []\n",
    "\n",
    "        for i in range(num_generate):\n",
    "            x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "            prediction = self.model.predict(x, verbose=0)\n",
    "            index = np.argmax(prediction)\n",
    "            result = self.int2char[index]\n",
    "            sys.stdout.write(result)\n",
    "            text_generated.append(result)\n",
    "            pattern = np.append(pattern, index)\n",
    "            pattern = pattern[1:len(pattern)]\n",
    "        print(\"\\n\")\n",
    "        answer = ''.join(text_generated)\n",
    "\n",
    "        return answer\n",
    "\n",
    "    def unlearn_gradient_reversal(self, canary_sequences):\n",
    "        \"\"\"\n",
    "        Apply gradient reversal technique to unlearn canary sequences.\n",
    "        \"\"\"\n",
    "        for seq in canary_sequences:\n",
    "            input_eval = [self.char2int[char] for char in seq]\n",
    "            input_eval = np.expand_dims(input_eval, 0)\n",
    "            target = [self.char2int[seq[-1]]]\n",
    "            target = to_categorical(target, num_classes=len(self.int2char))\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = self.model(input_eval)\n",
    "                loss = tf.keras.losses.categorical_crossentropy(target, predictions)\n",
    "            \n",
    "            grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "            neg_grads = [-grad for grad in grads]\n",
    "            self.model.optimizer.apply_gradients(zip(neg_grads, self.model.trainable_variables))\n",
    "\n",
    "    def unlearn_fine_tune(self, excluded_sequences):\n",
    "        \"\"\"\n",
    "        Retrain the model excluding specific sequences.\n",
    "        \"\"\"\n",
    "        excluded_indices = []\n",
    "        for seq in excluded_sequences:\n",
    "            seq_int = [self.char2int[char] for char in seq]\n",
    "            for i, pattern in enumerate(self.X):\n",
    "                if all(np.array_equal(pattern[j], seq_int[j]) for j in range(len(seq_int))):\n",
    "                    excluded_indices.append(i)\n",
    "\n",
    "        # Exclude the identified indices\n",
    "        X_new = np.delete(self.X, excluded_indices, axis=0)\n",
    "        y_new = np.delete(self.y, excluded_indices, axis=0)\n",
    "\n",
    "        self.model.fit(X_new, y_new, epochs=5, batch_size=64)\n",
    "\n",
    "    def unlearn_data_replacement(self, canary_sequences, replacement_sequences):\n",
    "        \"\"\"\n",
    "        Replace canary sequences with replacement sequences and retrain.\n",
    "        \"\"\"\n",
    "        for seq, replacement in zip(canary_sequences, replacement_sequences):\n",
    "            seq_int = [self.char2int[char] for char in seq]\n",
    "            replacement_int = [self.char2int[char] for char in replacement]\n",
    "\n",
    "            for i, pattern in enumerate(self.X):\n",
    "                if all(np.array_equal(pattern[j], seq_int[j]) for j in range(len(seq_int))):\n",
    "                    self.X[i] = np.array(replacement_int).reshape((self.seq_length, 1))\n",
    "                    self.y[i] = to_categorical(replacement_int[-1], num_classes=len(self.int2char))\n",
    "\n",
    "        self.model.fit(self.X, self.y, epochs=5, batch_size=64)\n",
    "\n",
    "    def unlearn_data_removal(self, canary_sequences):\n",
    "        \"\"\"\n",
    "        Remove canary sequences from the training data and retrain.\n",
    "        \"\"\"\n",
    "        for seq in canary_sequences:\n",
    "            seq_int = [self.char2int[char] for char in seq]\n",
    "            for i, pattern in enumerate(self.X):\n",
    "                if all(np.array_equal(pattern[j], seq_int[j]) for j in range(len(seq_int))):\n",
    "                    self.X = np.delete(self.X, i, axis=0)\n",
    "                    self.y = np.delete(self.y, i, axis=0)\n",
    "\n",
    "        self.model.fit(self.X, self.y, epochs=5, batch_size=64)\n",
    "    \n",
    "    def unlearn_canary(self, data_path, seq_length, n_canaries, tau, order, batch_size, scale, damping, iterations,\n",
    "                       replace_char, rounds=1, train_reduction=1.0, epochs=1, eval_reduction=None, stabilization_epochs=0,\n",
    "                       mixing_ratio=1.0, verbose=False):\n",
    "        chars_to_predict = 80\n",
    "        if verbose:\n",
    "            print('Testing canary before unlearning step ...')\n",
    "            pp_start, loss_start, acc_start, _ = self.test_canary(reference_char=replace_char,\n",
    "                                                                  chars_to_predict=chars_to_predict,\n",
    "                                                                  train_reduction=eval_reduction)\n",
    "        else:\n",
    "            pp_start, loss_start, acc_start = -1, -1, -1\n",
    "\n",
    "        print(f'canary {self.canary_number}')\n",
    "        indices_to_change, x_delta, y_delta = get_z_delta(self.X, data_path, self.canary_number, seq_length,\n",
    "                                                          self.int2char, n_canaries, replace_char)\n",
    "        if train_reduction != 1:\n",
    "            x_train_old = self.X.copy()\n",
    "            y_train_old = self.y.copy()\n",
    "            z_x_old, z_y_old = self.X[indices_to_change].copy(), self.y[indices_to_change].copy()\n",
    "            idx_train_2_idx_delta = {i: j for i, j in zip(indices_to_change, range(x_delta.shape[0]))}\n",
    "            self.reduce_train_set(train_reduction, delta_idx=indices_to_change)\n",
    "            # map the indices that were chosen back to the indices of x_delta\n",
    "            indices_delta_reduced = np.array([idx_train_2_idx_delta[idx] for idx in\n",
    "                                    self.new_train_indices[self.delta_idx_train]])\n",
    "            z_x_reduced = z_x_old[indices_delta_reduced]\n",
    "            z_y_reduced = z_y_old[indices_delta_reduced]\n",
    "            z_x_delta_reduced = x_delta[indices_delta_reduced]\n",
    "            z_y_delta_reduced = y_delta[indices_delta_reduced]\n",
    "            self.update_influence_variables_samples(z_x_reduced, z_y_reduced, z_x_delta_reduced, z_y_delta_reduced)\n",
    "            x_fixed, y_fixed = self.X.copy(), self.y.copy()\n",
    "            x_fixed[self.delta_idx_train] = z_x_delta_reduced\n",
    "            y_fixed[self.delta_idx_train] = z_y_delta_reduced\n",
    "        else:\n",
    "            self.update_influence_variables_samples_indices(indices_to_change, x_delta, y_delta)\n",
    "            x_fixed, y_fixed = self.X.copy(), self.y.copy()\n",
    "            x_fixed[indices_to_change] = x_delta\n",
    "            y_fixed[indices_to_change] = y_delta\n",
    "        start_time = time.time()\n",
    "        if order > 0:\n",
    "            theta_updated, diverged = self.approx_retraining(hvp_x=x_fixed, hvp_y=y_fixed, batch_size=batch_size,\n",
    "                                                             scale=scale,\n",
    "                                                             damping=damping, iterations=iterations, verbose=verbose,\n",
    "                                                             rounds=rounds, tau=tau, order=order)\n",
    "            if stabilization_epochs > 0:\n",
    "                assert not diverged\n",
    "                self.test_canary(reference_char=replace_char, weights=theta_updated,\n",
    "                                 chars_to_predict=chars_to_predict,\n",
    "                                 train_reduction=eval_reduction)\n",
    "                self.model.set_weights(theta_updated)\n",
    "                theta_updated, diverged = self.iter_approx_retraining(self.X, self.y,\n",
    "                                                                      x_fixed, y_fixed, indices_to_change,\n",
    "                                                                      prioritize_misclassified=True,\n",
    "                                                                      steps=stabilization_epochs,\n",
    "                                                                      verbose=False,\n",
    "                                                                      batch_size=batch_size, scale=scale,\n",
    "                                                                      damping=damping, iterations=iterations,\n",
    "                                                                      rounds=rounds, tau=tau, order=order,\n",
    "                                                                      mixing_ratio=mixing_ratio)\n",
    "        else:\n",
    "            theta_updated = self.fine_tune(x_fixed, y_fixed, learning_rate=tau, batch_size=batch_size, epochs=epochs)\n",
    "            diverged = False\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        print(f'Unlearning took {total_time} seconds.')\n",
    "        if train_reduction != 1:\n",
    "            self.reduce_train_set(x_train_old=x_train_old, y_train_old=y_train_old)\n",
    "        pp_end, loss_end, acc_end, completion = self.test_canary(reference_char=replace_char, weights=theta_updated,\n",
    "                                                                 chars_to_predict=chars_to_predict,\n",
    "                                                                 train_reduction=eval_reduction)\n",
    "        return theta_updated, pp_start, pp_end, loss_start, loss_end, acc_start, acc_end, diverged, completion, total_time\n",
    "\n",
    "    def approx_retraining(self, **kwargs):\n",
    "        batch_size = kwargs.get('batch_size', 500)\n",
    "        scale = kwargs.get('scale', 10)\n",
    "        damping = kwargs.get('damping', 0.1)\n",
    "        iterations = kwargs.get('iterations', -1)\n",
    "        verbose = kwargs.get('verbose', False)\n",
    "        rounds = kwargs.get('rounds', 1)\n",
    "        conjugate_gradients = kwargs.get('cg', False)\n",
    "        order = kwargs.get('order', 2)\n",
    "        tau = kwargs.get('tau', 1)  # unlearning rate\n",
    "        hvp_x = kwargs.get('hvp_x', self.x_train)\n",
    "        hvp_y = kwargs.get('hvp_y', self.y_train)\n",
    "\n",
    "        if order == 1:\n",
    "            # First-order update\n",
    "            diff = self.get_gradients_diff(self.z_x, self.z_y, self.z_x_delta, self.z_y_delta)\n",
    "            d_theta = diff\n",
    "            diverged = False\n",
    "        elif order == 2:\n",
    "            # Second-order update\n",
    "            diff = self.get_gradients_diff(self.z_x, self.z_y, self.z_x_delta, self.z_y_delta)\n",
    "            # Skip HVP if diff == 0\n",
    "            if np.sum(np.sum(d) for d in diff) == 0:\n",
    "                d_theta = diff\n",
    "                diverged = False\n",
    "            elif conjugate_gradients:\n",
    "                d_theta = self.get_inv_hvp_cg(diff, damping)\n",
    "                diverged = True\n",
    "            else:\n",
    "                d_theta, diverged = self.get_inv_hvp_lissa(\n",
    "                    hvp_x, hvp_y, diff, batch_size, scale, damping, iterations, verbose, rounds)\n",
    "\n",
    "        if order != 0:\n",
    "            # Only update trainable weights (non-invasive workaround for BatchNorm layers in CIFAR model)\n",
    "            d_theta = [d_theta.pop(0) if w.trainable else 0 for w in self.model.weights]\n",
    "            theta_approx = [w - tau * d_t for w, d_t in zip(self.model.get_weights(), d_theta)]\n",
    "        else:\n",
    "            theta_approx = self.model.get_weights()\n",
    "            diverged = False\n",
    "\n",
    "        return theta_approx, diverged\n",
    "    \n",
    "    def gradient_step(self, x, y, learning_rate):\n",
    "        \"\"\"\n",
    "        Perform a single gradient-descent-step based on x and y.\n",
    "        \"\"\"\n",
    "        model_params = self.model.get_weights()\n",
    "        grads = self.get_gradients(x, y)\n",
    "        new_weights = [w - learning_rate * g for w, g in zip(model_params, grads)]\n",
    "        return new_weights\n",
    "\n",
    "    def get_gradients(self, x, y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.model(x, training=True)\n",
    "            loss = tf.keras.losses.categorical_crossentropy(y, predictions)\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        return grads\n",
    "    \n",
    "    # Function to evaluate unlearning technique\n",
    "    def evaluate_unlearning(self, canary_sequences, technique, **kwargs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Apply the specified unlearning technique\n",
    "        if technique == 'gradient_reversal':\n",
    "            self.unlearn_gradient_reversal(canary_sequences)\n",
    "        elif technique == 'fine_tune':\n",
    "            self.unlearn_fine_tune(canary_sequences)\n",
    "        elif technique == 'data_replacement':\n",
    "            replacement_sequences = kwargs.get('replacement_sequences', [])\n",
    "            self.unlearn_data_replacement(canary_sequences, replacement_sequences)\n",
    "        elif technique == 'data_removal':\n",
    "            self.unlearn_data_removal(canary_sequences)\n",
    "        elif technique == 'canary':\n",
    "            self.unlearn_canary(**kwargs)\n",
    "        elif technique == 'gradient_step':\n",
    "            x = kwargs.get('x')\n",
    "            y = kwargs.get('y')\n",
    "            learning_rate = kwargs.get('learning_rate')\n",
    "            self.gradient_step(x, y, learning_rate)\n",
    "        elif technique == 'get_gradients':\n",
    "            x = kwargs.get('x')\n",
    "            y = kwargs.get('y')\n",
    "            self.get_gradients(x, y)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "\n",
    "        start_string = \"`my telephone number is \"\n",
    "        genereted_text = self.generate_text(start_string=start_string)\n",
    "        \n",
    "\n",
    "        # Calculate and print perplexity\n",
    "        perplexity = self.calculate_perplexity(answer=genereted_text)\n",
    "        print(f\"Perplexity after {technique}: {perplexity}\")\n",
    "\n",
    "        # Calculate and print approximate exposure\n",
    "        approximate_exposure = self.approx_exposure(genereted_text)\n",
    "        print(f\"Approximate Exposure after {technique}: {approximate_exposure}\")\n",
    "\n",
    "        print(f\"Time taken for {technique}: {total_time} seconds\")\n",
    "\n",
    "    def calc_sequence_perplexity(self, sequence, start_sequence):\n",
    "        number_char_indices = [self.char2int[n] for n in sequence]\n",
    "        print(f'Number char indices: {number_char_indices}')\n",
    "        start_seq = np.array([self.char2int[s] for s in start_sequence], dtype=np.int8)\n",
    "        start_seq = start_seq.reshape((1, len(start_seq), 1))\n",
    "        digit_distribution = np.zeros(len(sequence))\n",
    "\n",
    "        argmax_chars = ''\n",
    "        for i in range(len(sequence)):\n",
    "            index_distribution = self.model.predict(start_seq, verbose=0)\n",
    "            char_index = np.argmax(index_distribution)\n",
    "            digit_distribution[i] = index_distribution[0, number_char_indices[i]]\n",
    "            start_seq = np.append(start_seq, char_index.reshape((1, 1, 1)), axis=1)\n",
    "            start_seq = start_seq[:, 1:start_seq.shape[1]+1, :]\n",
    "            argmax_chars += self.int2char[char_index]\n",
    "\n",
    "        print(f' Seed: {self.canary_start}')\n",
    "        print(f' Predicted sequence: {argmax_chars}')\n",
    "        print(f' Digit distribution: {digit_distribution}')\n",
    "        print('Canary perplexities at all poins: ')\n",
    "\n",
    "        for i in range(1, len(sequence)+1):\n",
    "            print(f'Perplexity at position {i}: {-np.sum(np.log2(digit_distribution[:i]))}')\n",
    "        sequence_perplexity = -np.sum(np.log2(digit_distribution))\n",
    "        return sequence_perplexity\n",
    "\n",
    "\n",
    "\n",
    "    def calculate_perplexity(self, no_samples=100000, plot=False, only_digits=False, answer=None):\n",
    "        #extract number in canary\n",
    "        answer_number = [int(s) for s in answer if s.isdigit()]\n",
    "        print(f' Numbers founded: {answer_number}')\n",
    "        self.canary_number = answer_number\n",
    "        model = self.model\n",
    "        if only_digits:\n",
    "            numbers = np.unique([d for d in self.canary_number])\n",
    "            char_indices = [self.char2int[n] for n in numbers]\n",
    "        else:\n",
    "            char_indices = list(self.int2char.keys())\n",
    "        len_canary = len(self.canary_number)\n",
    "        start_seq = np.array([self.char2int[s] for s in self.canary_start], dtype=np.int8)\n",
    "        start_seq = start_seq.reshape((1, len(start_seq)))\n",
    "        start_seq = np.repeat(start_seq, no_samples, axis=0)\n",
    "        random_sequences = np.random.choice(char_indices, size=(no_samples, len_canary), replace=True)\n",
    "        perplexities = np.zeros(no_samples)\n",
    "\n",
    "        for i in range(random_sequences.shape[1]):\n",
    "            # add a random char to the sequences to predict them\n",
    "            start_seq = np.insert(start_seq, start_seq.shape[1], random_sequences[:, i], axis=1)\n",
    "            # extract proba of inserted chars\n",
    "            start_seq_pred = model.predict(start_seq, batch_size=1000, verbose=1)[range(no_samples), random_sequences[:, i]]\n",
    "            perplexities -= np.log2(start_seq_pred)\n",
    "        perplexities = np.array(perplexities)\n",
    "\n",
    "                # Check for and handle infinite values\n",
    "        if np.any(np.isinf(perplexities)):\n",
    "            print(\"Found infinite perplexity values, handling them...\")\n",
    "            max_finite_value = np.max(perplexities[np.isfinite(perplexities)])\n",
    "            perplexities[np.isinf(perplexities)] = max_finite_value\n",
    "        \n",
    "        if plot:\n",
    "            _, bins, _ = plt.hist(perplexities, bins=1000, density=True)\n",
    "            ae, loc, scale = skewnorm.fit(perplexities)\n",
    "            print('Skewnorm-fit parameters: {0:.3f} - {1:.3f} - {2:.3f}'.format(ae, loc, scale))\n",
    "            sn = skewnorm(ae, loc, scale)\n",
    "            x = np.linspace(0, max(bins), 500)\n",
    "            plt.plot(x, sn.pdf(x), linewidth=5.0, label=\"Skewnorm-fit\")\n",
    "            plt.xlabel('Log Perplexity')\n",
    "            plt.ylabel('Relative Frequency')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        return perplexities\n",
    "\n",
    "    def approx_exposure(self, perplexity_values, perplexities, only_digits=False, canary_number=None):\n",
    "        self.canary_number = canary_number\n",
    "        if type(perplexity_values) is not list:\n",
    "            perplexity_values = [perplexity_values]\n",
    "        if only_digits:\n",
    "            numbers = np.unique([d for d in self.canary_number])\n",
    "            char_indices = [self.char2int[n] for n in numbers]\n",
    "        else:\n",
    "            char_indices = list(self.int2char.keys())\n",
    "        n_combinations = len(char_indices) ** len(self.canary_number)  # total number of combinations possible\n",
    "        R = np.log2(float(n_combinations))  # if n_combinations is large np.log2 needs needs float\n",
    "        ae, loc, scale = skewnorm.fit(perplexities)\n",
    "        sn = skewnorm(ae, loc, scale)\n",
    "        quantiles = [sn.cdf(pv) for pv in perplexity_values]\n",
    "        exposures = [-np.log2(q) for q in quantiles]\n",
    "        for i in range(len(perplexity_values)):\n",
    "            print('Results for {}'.format(perplexity_values[i]))\n",
    "            print('{}% of all sequences are more likely than the given one.'.format(quantiles[i] * 100))\n",
    "            print('Log(|R|) is {}'.format(R))\n",
    "            print('The exposure of the sequence is {}'.format(exposures[i]))\n",
    "        return exposures\n",
    "            \n",
    "    def update_influence_variables_samples_indices(self, indices_to_change, new_x, new_y=None):\n",
    "        \"\"\"\n",
    "        Update the influence variables for the samples at the specified indices.\n",
    "        \"\"\"\n",
    "\n",
    "        assert np.min(indices_to_change) >= 0 and np.max(indices_to_change) < len(self.X)\n",
    "        assert self.X[indices_to_change].shape == new_x.shape, '{} != {}'.format(\n",
    "            self.X[indices_to_change].shape, new_x.shape\n",
    "        )\n",
    "        self.z_x, self.z_y = self.X[indices_to_change], self.y[indices_to_change]\n",
    "        self.z_x_delta = new_x\n",
    "        if new_y is not None:\n",
    "            assert self.y[indices_to_change].shape == new_y.shape\n",
    "            self.z_y_delta = new_y\n",
    "\n",
    "        else:\n",
    "            self.z_y_delta = self.y[indices_to_change]\n",
    "        if sp.issparse(self.X):\n",
    "            self.z_x, self_z_x_delta = self.z_x.toarray(), self.z_x_delta.toarray()\n",
    "    def update_influence_variables_samples(self, z_x, z_y, z_x_delta, z_y_delta=None):\n",
    "        self.z_x = z_x\n",
    "        self.z_y = z_y\n",
    "        self.z_x_delta = z_x_delta\n",
    "        self.z_y_delta = z_y_delta if z_y_delta is not None else z_y\n",
    "        if sp.issparse(self.X):\n",
    "            self.z_x, self.z_x_delta = self.z_x.toarray(), self.z_x_delta.toarray()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Prepare the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-11 18:05:24.896330: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2024-07-11 18:05:24.896415: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: conite-HP\n",
      "2024-07-11 18:05:24.896430: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: conite-HP\n",
      "2024-07-11 18:05:24.896667: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 555.42.2\n",
      "2024-07-11 18:05:24.896734: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 555.42.2\n",
      "2024-07-11 18:05:24.896749: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 555.42.2\n",
      "2024-07-11 18:05:24.897268: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 64)          3456      \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, None, 512)         1181696   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 512)         0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, None, 512)         2099200   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, None, 512)         0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 512)               2099200   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 54)                27702     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,411,254\n",
      "Trainable params: 5,411,254\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "unique characters :  ['\\n', ' ', '!', '\"', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '[', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Number of unique characters:  54\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sample_text = 'This is a sample text for training our LSTM model. We will use this text to train our model.'\n",
    "CANARY_STR = \"`my telephone number is {}!' said alice.\\n\\n  \"\n",
    "CANARY_START = \"`my telephone number is \"\n",
    "canary_number = \"0123456789\"\n",
    "canary = CANARY_STR.format(canary_number)\n",
    "\n",
    "filename = '../train_test_data/Alice/alice_in_wonderland.txt'\n",
    "seq_length = 24\n",
    "canary_insertions = 10\n",
    "vocab_size=54\n",
    "embedding_dim=64\n",
    "lstm_units=512\n",
    "batch_size=64\n",
    "epochs = 3\n",
    "\n",
    "text_model = TextModel(filename=filename, seq_length=seq_length, canary=canary, \n",
    "                        canary_insertions=canary_insertions, vocab_size=vocab_size,\n",
    "                        embedding_dim=embedding_dim, lstm_units=lstm_units,\n",
    "                        batch_size=batch_size, dropout_rate=0.5, canary_start=CANARY_START)\n",
    "X_train, y_train, int2char = text_model.X, text_model.y, text_model.int2char\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already trained. Loading...\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "# Check if folder models/unintended exists and create if it doesn't\n",
    "\n",
    "if not os.path.exists('unintended_models'):\n",
    "    os.makedirs('unintended_models')\n",
    "text_model.train(X_train, y_train, epochs=epochs, model_folder='unintended_models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Generate Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" `my telephone number is  \"\n",
      "Generation of text: \n",
      "\n",
      "0123456789!' said alice.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_string = CANARY_START\n",
    "response = text_model.generate_text(start_string, num_generate=25, temperature=1.0)\n",
    "canary_number = \"0123456789\"\n",
    "\n",
    "# unlearner = RNNUNlearner(model=text_model.model, x_train=X_train, y_train=y_train, embedding_dim=embedding_dim, idx2char=int2char, canary_start=CANARY_START, canary_number=canary_number, canary_repetitions=canary_insertions)\n",
    "# unlearner.generate_data(start_str=start_string, modelpath='unintended_models/final_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Computing of Log perplexity**\n",
    "\n",
    "We see that the model predicts the canary sequence and reveals the telephone number. Next let us quantify how strong the number is memorized by computing the log perplexity of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"0123456789!' said alice.\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number char indices: [11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      " Seed: my telephone number is \n",
      " Predicted sequence: 0123456789\n",
      " Digit distribution: [0.99838424 0.99996269 0.99982268 0.99999213 0.99992943 0.99999428\n",
      " 0.99990356 0.99998713 0.99998534 0.99999988]\n",
      "Canary perplexities at all poins: \n",
      "Perplexity at position 1: 0.0023329380949429187\n",
      "Perplexity at position 2: 0.0023867696689647317\n",
      "Perplexity at position 3: 0.0026426165466662327\n",
      "Perplexity at position 4: 0.0026539674462766148\n",
      "Perplexity at position 5: 0.0027557847683505627\n",
      "Perplexity at position 6: 0.0027640399592102904\n",
      "Perplexity at position 7: 0.0029031806332513845\n",
      "Perplexity at position 8: 0.0029217548791128476\n",
      "Perplexity at position 9: 0.002942908900257336\n",
      "Perplexity at position 10: 0.0029430808829184496\n"
     ]
    }
   ],
   "source": [
    "# compute the perplexity of the canary\n",
    "sequence_perplexity = text_model.calc_sequence_perplexity(sequence=canary_number, start_sequence=CANARY_START)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's confidence in case of `original notebook `in predicting the canary sequence diminishes as it progresses through the digits, as indicated by the increasing perplexity values.\n",
    "The log perplexity value of 13 suggests that, overall, the model struggles significantly to predict the full canary sequence accurately.\n",
    "These results help us understand how well the model can generate or predict a given sequence, with lower perplexity indicating better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About of this results**\n",
    "\n",
    "The `digit distribution values are very high`, close to 1, indicating that the model is very confident in predicting each digit.\n",
    "The canary perplexities at each position are extremely low (all below 0.003), which indicates that the model predicts each digit in the sequence with high confidence and accuracy.\n",
    "Overall, the model performs exceptionally well in predicting the sequence \"0123456789\" given the seed text, as evidenced by the high probabilities and low perplexities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Calculer la distribution of Perplexity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Numbers founded: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "100/100 [==============================] - 37s 364ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages/ipykernel_launcher.py:456: RuntimeWarning: divide by zero encountered in log2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 38s 373ms/step\n",
      "100/100 [==============================] - 32s 314ms/step\n",
      "100/100 [==============================] - 28s 280ms/step\n",
      "100/100 [==============================] - 28s 278ms/step\n",
      "100/100 [==============================] - 28s 282ms/step\n",
      "100/100 [==============================] - 31s 310ms/step\n",
      "100/100 [==============================] - 33s 331ms/step\n",
      "100/100 [==============================] - 33s 328ms/step\n",
      "100/100 [==============================] - 33s 330ms/step\n",
      "Found infinite perplexity values, handling them...\n",
      "Skewnorm-fit parameters: 7.870 - 307.217 - 365.883\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perplexities = text_model.calculate_perplexity(answer=response, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Measuring Exposure**\n",
    "\n",
    "Use the test_canary and calc_sequence_perplexity functions to evaluate the exposure of the canary sequence:\n",
    "\n",
    "This step involves evaluating how much the model has retained the canary sequences. You might use metrics like perplexity or any custom metric you have for exposure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canary_exposure = text_model.approx_exposure(perplexity_values=[sequence_perplexity], perplexities=perplexities, only_digits=True, canary_number=canary_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage of Sequences More Likely Than the Given One:\n",
    "\n",
    "`1.9874646935000676e-11% `of all sequences are more likely than the given one.\n",
    "This extremely small percentage indicates that the canary sequence is highly unique and likely very well memorized by the model, as almost no other sequences are more probable.\n",
    "Log(|R|):\n",
    "\n",
    "`Log(|R|) is 33.219280948873624`\n",
    "|R| represents the number of all possible sequences. The logarithm of this number gives us a sense of the sequence space's size. A large value here means there are many possible sequences, indicating a vast space of possible outputs.\n",
    "Exposure:\n",
    "\n",
    "`The exposure of the sequence is 42.194136001800246`\n",
    "Exposure is a metric proposed by Carlini et al. to quantify memorization. It essentially measures how easy it is to extract the canary sequence from the model.\n",
    "An exposure value greater than 20 is typically considered to indicate significant memorization. Your value of 42.194136001800246 is much higher, which strongly suggests that the canary number is memorized by the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Apply Unlearning Techniques**\n",
    "\n",
    "\n",
    "You have different methods for unlearning: gradient reversal, fine-tuning, data replacement, and data removal. Choose one or combine them as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "\n",
    "Your results show a strong memorization of the canary number by the model. The high exposure value indicates that the sequence is not only memorized but also easily retrievable. This suggests that the model has overfitted to the canary sequence, making it stand out in the vast space of possible sequences. This kind of analysis is crucial for understanding the privacy risks associated with language models and their potential to inadvertently memorize and expose sensitive information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "techniques = ['gradient_reversal', 'fine_tune', 'data_replacement', 'data_removal', 'canary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Applications.CanaryRemoval.CanaryRemoval import unlearn_canary\n",
    "\n",
    "replacement_strs = ['not there ', 'dry enough']\n",
    "\n",
    "# first order params\n",
    "taus = [0.0003, 0.0009]\n",
    "batch_size_fo = 64\n",
    "data_path = '../train_test_data/Alice/alice_in_wonderland.txt'\n",
    "orders = [1, 2]\n",
    "# second order params\n",
    "batch_size_so = 500\n",
    "damping = 0.1\n",
    "iterations = 30\n",
    "scales = [39000, 36000]\n",
    "canary_repetitions = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform the unlearning for the different replacements and orders\n",
    "\n",
    "### First Order - Replacement \"not there \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 64)          3456      \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, None, 256)         328704    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, None, 256)         65792     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, None, 256)         0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 256)               525312    \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 54)                13878     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,002,934\n",
      "Trainable params: 1,002,934\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "unlearner = RNNUNlearner(model=text_model.model, x_train=X_train, y_train=y_train, embedding_dim=embedding_dim, idx2char=int2char, canary_start=CANARY_START, canary_number=canary_number, canary_repetitions=canary_insertions)\n",
    "\n",
    "unlearner.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " idx2char {0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: \"'\", 5: '(', 6: ')', 7: '*', 8: ',', 9: '-', 10: '.', 11: '0', 12: '1', 13: '2', 14: '3', 15: '4', 16: '5', 17: '6', 18: '7', 19: '8', 20: '9', 21: ':', 22: ';', 23: '?', 24: '[', 25: ']', 26: '_', 27: '`', 28: 'a', 29: 'b', 30: 'c', 31: 'd', 32: 'e', 33: 'f', 34: 'g', 35: 'h', 36: 'i', 37: 'j', 38: 'k', 39: 'l', 40: 'm', 41: 'n', 42: 'o', 43: 'p', 44: 'q', 45: 'r', 46: 's', 47: 't', 48: 'u', 49: 'v', 50: 'w', 51: 'x', 52: 'y', 53: 'z'}\n",
      "85090 of 148805 samples are affected by the unlearning (57.18221833943752%)\n",
      "Unlearning took 135.75469064712524 seconds.\n",
      "Seed: `my telephone number is \n",
      "Prediction: the same thing as \"i sleep i had somewhened at once, and then another confusion \n",
      "Train loss: [0.370584636926651, 0.878700315952301]\n",
      "Train acc: 0.8787003124894996\n",
      "Digit probas: [2.75076687e-01 1.37412330e-21 1.19164003e-26 6.96245263e-19\n",
      " 4.81292950e-10 4.86835724e-13 1.67999565e-25 6.49357142e-35\n",
      " 0.00000000e+00 1.31528211e-17]\n",
      "Replacement_char proba: [2.17095111e-03 1.30121776e-07 4.20175526e-11 5.07696939e-04\n",
      " 1.51835603e-03 1.50976749e-02 3.85270193e-02 7.38681800e-18\n",
      " 2.91385419e-07 5.37968335e-05]\n",
      "Canary perplexities at all points:\n",
      "1: 1.8620942180102091\n",
      "2: 71.16407274942588\n",
      "3: 157.28125471978612\n",
      "4: 217.59829291611902\n",
      "5: 248.55065857437833\n",
      "6: 289.452288772087\n",
      "7: 371.7520336428269\n",
      "8: 485.3205047955998\n",
      "9: inf\n",
      "10: inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../Unlearner/RNNUnlearner.py:256: RuntimeWarning: divide by zero encountered in log2\n",
      "  print('{}: {}'.format(j, -np.sum(np.log2(digit_distribution[:j, 0]))))\n",
      "../Unlearner/RNNUnlearner.py:257: RuntimeWarning: divide by zero encountered in log2\n",
      "  canary_perplexity = -np.sum(np.log2(digit_distribution[:, 0]))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "res = unlearn_canary(unlearner=unlearner, data_path=data_path, seq_length=seq_length, tau=taus[0], order=1, batch_size=batch_size_fo,\n",
    "                     scale=1, damping=0.0, iterations=1, replace_char=replacement_strs[0], n_canaries=canary_repetitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" `my telephone number is  \"\n",
      "Generation of text: \n",
      "\n",
      "the same thing as \"i sleep i had somewhened at once\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = text_model.generate_text(start_string, num_generate=51, temperature=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Order - Replacement \"not there \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " idx2char {0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: \"'\", 5: '(', 6: ')', 7: '*', 8: ',', 9: '-', 10: '.', 11: '0', 12: '1', 13: '2', 14: '3', 15: '4', 16: '5', 17: '6', 18: '7', 19: '8', 20: '9', 21: ':', 22: ';', 23: '?', 24: '[', 25: ']', 26: '_', 27: '`', 28: 'a', 29: 'b', 30: 'c', 31: 'd', 32: 'e', 33: 'f', 34: 'g', 35: 'h', 36: 'i', 37: 'j', 38: 'k', 39: 'l', 40: 'm', 41: 'n', 42: 'o', 43: 'p', 44: 'q', 45: 'r', 46: 's', 47: 't', 48: 'u', 49: 'v', 50: 'w', 51: 'x', 52: 'y', 53: 'z'}\n",
      "85090 of 148805 samples are affected by the unlearning (57.18221833943752%)\n",
      "Unlearning took 191.4133825302124 seconds.\n",
      "Seed: `my telephone number is \n",
      "Prediction: the same thing as \"i sleep i had somewhened at once, and then another confusion \n",
      "Train loss: [0.3705858886241913, 0.8787070512771606]\n",
      "Train acc: 0.8787070326937939\n",
      "Digit probas: [2.73215264e-01 1.37990973e-21 1.19516472e-26 6.96527900e-19\n",
      " 4.80795292e-10 4.87229994e-13 1.67897285e-25 6.49168936e-35\n",
      " 0.00000000e+00 1.31352799e-17]\n",
      "Replacement_char proba: [1.19068281e-04 2.79646046e-11 6.45893393e-17 3.58492208e-10\n",
      " 3.47419009e-02 2.01616331e-05 5.27829630e-04 9.13547586e-16\n",
      " 1.23895213e-14 5.78606129e-03]\n",
      "Canary perplexities at all points:\n",
      "1: 1.871890008037621\n",
      "2: 71.16780611236116\n",
      "3: 157.28072711730033\n",
      "4: 217.5971797776504\n",
      "5: 248.5510379560291\n",
      "6: 289.4515002416188\n",
      "7: 371.75212371500464\n",
      "8: 485.3210130695532\n",
      "9: inf\n",
      "10: inf\n"
     ]
    }
   ],
   "source": [
    "res = unlearn_canary(unlearner=unlearner, data_path=data_path, seq_length=seq_length, tau=taus[1], order=2, batch_size=batch_size_so, scale=scales[1], damping=damping, iterations=iterations, replace_char=replacement_strs[1], n_canaries=canary_repetitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Canary Perplexities:` These values measure how well the model predicts the canary (a known sequence inserted into the data) at different points. Higher perplexity indicates the model is less certain about the sequence.\n",
    "Infinite Perplexity: For positions 9 and 10, the perplexity is infinite, indicating the model failed to predict these positions accurately.\n",
    "\n",
    "\n",
    "`Summary and Interpretation`\n",
    "**Character Index Mapping**: Maps indices to characters for decoding model predictions.\n",
    "\n",
    "**Unlearning Impact**: A significant portion of the data was affected by unlearning.\n",
    "\n",
    "**Seed and Prediction**: The seed string and the model's generated text, which seems incoherent.\n",
    "\n",
    "**Train Loss and Accuracy**: Training loss and accuracy after unlearning indicate model performance.\n",
    "\n",
    "**Digit and Replacement Character Probabilities**: The model's uncertainty in predicting digits and replacement characters.\n",
    "\n",
    "**Canary Perplexities**: High perplexity values, especially infinite ones, suggest the model has difficulty with certain parts of the canary sequence, indicating successful unlearning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Canary :  \"my bank account number is 123456789\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#fd\n",
    "# sample_text = 'This is a sample text for training our LSTM model. We will use this text to train our model.'\n",
    "CANARY_STR = \"`my bank account number is {}!' said alice.\\n\\n  \"\n",
    "CANARY_START = \"`my bank account number is \"\n",
    "canary_number = \"ae12345I6eit4o78ert90\"\n",
    "canary = CANARY_STR.format(canary_number)\n",
    "\n",
    "filename = '../train_test_data/Alice/alice_in_wonderland.txt'\n",
    "seq_length = 24\n",
    "canary_insertions = 1\n",
    "vocab_size=54\n",
    "embedding_dim=64\n",
    "lstm_units=512\n",
    "batch_size=64\n",
    "epochs = 100\n",
    "\n",
    "text_model = TextModel(filename=filename, seq_length=seq_length, canary=canary, \n",
    "                        canary_insertions=canary_insertions, vocab_size=vocab_size,\n",
    "                        embedding_dim=embedding_dim, lstm_units=lstm_units,\n",
    "                        batch_size=batch_size, dropout_rate=0.5, canary_start=CANARY_START)\n",
    "X_train, y_train, int2char = text_model.X, text_model.y, text_model.int2char\n",
    "\n",
    "\n",
    "# Train the model\n",
    "# Check if folder models/unintended exists and create if it doesn't\n",
    "\n",
    "if not os.path.exists('other_unintended_models'):\n",
    "    os.makedirs('other_unintended_models')\n",
    "text_model.train(X_train, y_train, epochs=epochs, model_folder='other_unintended_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_string = CANARY_START\n",
    "response = text_model.generate_text(start_string, num_generate=25, temperature=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the perplexity of the canary\n",
    "sequence_perplexity = text_model.calc_sequence_perplexity(sequence=canary_number, start_sequence=CANARY_START)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexities = text_model.calculate_perplexity(answer=response, plot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canary_exposure = text_model.approx_exposure(perplexity_values=[sequence_perplexity], perplexities=perplexities, only_digits=True, canary_number=canary_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Applications.CanaryRemoval.CanaryRemoval import unlearn_canary\n",
    "\n",
    "replacement_strs = ['not there ', 'dry enough']\n",
    "\n",
    "# first order params\n",
    "taus = [0.0003, 0.0009]\n",
    "batch_size_fo = 64\n",
    "data_path = '../train_test_data/Alice/alice_in_wonderland.txt'\n",
    "orders = [1, 2]\n",
    "# second order params\n",
    "batch_size_so = 500\n",
    "damping = 0.1\n",
    "iterations = 30\n",
    "scales = [39000, 36000]\n",
    "canary_repetitions = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlearner = RNNUNlearner(model=text_model.model, x_train=X_train, y_train=y_train, embedding_dim=embedding_dim, idx2char=int2char, canary_start=CANARY_START, canary_number=canary_number, canary_repetitions=canary_insertions)\n",
    "unlearner.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform the unlearning for the different replacements and orders\n",
    "\n",
    "### First Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlearner = RNNUNlearner(model=text_model.model, x_train=X_train, y_train=y_train, embedding_dim=embedding_dim, idx2char=int2char, canary_start=CANARY_START, canary_number=canary_number, canary_repetitions=canary_insertions)\n",
    "\n",
    "unlearner.model.summary()\n",
    "\n",
    "res = unlearn_canary(unlearner=unlearner, data_path=data_path, seq_length=seq_length, tau=taus[0], order=1, batch_size=batch_size_fo,\n",
    "                     scale=1, damping=0.0, iterations=1, replace_char=replacement_strs[0], n_canaries=canary_repetitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Order - Replacement \"not there \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = unlearn_canary(unlearner=unlearner, data_path=data_path, seq_length=seq_length, tau=taus[1], order=2, batch_size=batch_size_so, scale=scales[1], damping=damping, iterations=iterations, replace_char=replacement_strs[1], n_canaries=canary_repetitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
