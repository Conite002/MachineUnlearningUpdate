{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Build and Train the Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Embedding, LSTM, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger,  EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import skewnorm\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from Applications.CanaryRemoval.CanaryRemoval import unlearn_canary, get_z_delta\n",
    "from Unlearner.RNNUnlearner import RNNUNlearner\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch < 5:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return 0.0001\n",
    "\n",
    "\n",
    "class TextModel:\n",
    "    def __init__(self, filename, seq_length, canary, canary_insertions, vocab_size, embedding_dim, lstm_units, batch_size, dropout_rate=0.2):\n",
    "        self.filename = filename\n",
    "        self.seq_length = seq_length\n",
    "        self.canary = canary\n",
    "        self.canary_insertions = canary_insertions\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lstm_units = lstm_units\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.int2char = {}\n",
    "        self.char2int = {}\n",
    "\n",
    "        self.model = self.build_model()\n",
    "        self.X, self.y, self.int2char = self.load_data()\n",
    "\n",
    "    def set_char2int(self, int2char):\n",
    "        self.char2int = {v: k for k, v in int2char.items()}\n",
    "        \n",
    "    def load_data(self):\n",
    "        np.random.seed(42)\n",
    "        raw_text = open(self.filename, 'r', encoding='utf-8').read()[265:]  # Charger le texte brut (en supposant que le début est ignoré)\n",
    "        raw_text = self.insert_canary(raw_text)  # Insérer le canary dans le texte brut\n",
    "        raw_text = raw_text.lower()  # Convertir en minuscules\n",
    "        chars = sorted(list(set(raw_text)))  # Obtenir tous les caractères uniques dans le texte\n",
    "\n",
    "        print(\"unique characters : \", chars)\n",
    "        print(\"Number of unique characters: \", len(chars))\n",
    "\n",
    "        # Initialize char2int and int2char using the unique characters\n",
    "        for i, c in enumerate(chars):\n",
    "            self.char2int[c] = i\n",
    "            self.int2char[i] = c\n",
    "            \n",
    "        self.int2char = {i: c for i, c in enumerate(chars)}\n",
    "        self.set_char2int(self.int2char)\n",
    "            \n",
    "        n_chars = len(raw_text)\n",
    "        dataX = []\n",
    "        dataY = []\n",
    "        # Générer des paires d'entrée-sortie codées en entiers                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
    "        for i in range(0, n_chars - self.seq_length, 1):\n",
    "            seq_in = raw_text[i:i + self.seq_length]\n",
    "            seq_out = raw_text[i + self.seq_length]                                                                                                                                                                                                                                                                            \n",
    "            dataX.append([self.char2int[char] for char in seq_in])\n",
    "            dataY.append(self.char2int[seq_out])\n",
    "        n_patterns = len(dataX)\n",
    "        X = np.reshape(dataX, (n_patterns, self.seq_length, 1))\n",
    "        y = to_categorical(dataY)\n",
    "        return X, y, self.int2char\n",
    "\n",
    "    def insert_canary(self, text):\n",
    "        if self.canary_insertions == 0:\n",
    "            return text\n",
    "        canary_len = len(self.canary)  # Longueur du canary\n",
    "        breaks = [m.start() for m in re.finditer('\\n\\n  ', text)]\n",
    "        insertion_points = sorted(np.random.choice(breaks, self.canary_insertions, replace=False))\n",
    "        new_text = ''\n",
    "        for idx in range(len(insertion_points)):\n",
    "            point_pre = insertion_points[idx - 1] + canary_len if idx != 0 else 0\n",
    "            point_last = insertion_points[idx] + canary_len\n",
    "            new_text += text[point_pre:point_last] + self.canary\n",
    "        new_text += text[point_last:]\n",
    "        return new_text\n",
    "    \n",
    "    def find_insertion_points(self, text):\n",
    "        \"\"\"Find appropriate points to insert the canary string.\"\"\"\n",
    "        canary_len = len(self.canary)\n",
    "        breakpoints = [m.start() for m in re.finditer(r'\\s+', text)]\n",
    "        if len(breakpoints) < self.canary_insertions:\n",
    "            raise ValueError(\"Not enough breakpoints to insert the canary string the specified number of times.\")\n",
    "        insertion_points = sorted(random.sample(breakpoints, self.canary_insertions))\n",
    "        return insertion_points\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim))\n",
    "        model.add(LSTM(self.lstm_units, return_sequences=True))\n",
    "        model.add(Dense(self.lstm_units, activation='relu'))\n",
    "        model.add(Dropout(self.dropout_rate))\n",
    "        model.add(LSTM(self.lstm_units))\n",
    "        model.add(Dense(self.lstm_units, activation='relu'))\n",
    "        model.add(Dropout(self.dropout_rate))\n",
    "        model.add(Dense(self.vocab_size, activation='softmax'))\n",
    "        optimizer = Adam(learning_rate=0.001)\n",
    "        model.compile(loss=categorical_crossentropy, optimizer=optimizer, metrics=['accuracy'])\n",
    "        model.summary()\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def train(self, x_train, y_train, epochs, model_folder, customer_name):\n",
    "        customer_folder = os.path.join(model_folder, customer_name)\n",
    "        if not os.path.join(customer_folder):\n",
    "            os.makedirs(customer_folder)\n",
    "        \n",
    "           # Check if the model is already trained\n",
    "        if os.path.exists(os.path.join(customer_folder, 'final_model.h5')):\n",
    "            print(\"Model already trained. Loading...\")\n",
    "            self.model = load_model(os.path.join(customer_folder, 'final_model.h5'))\n",
    "            return\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "        \n",
    "        checkpoint_path = os.path.join(customer_folder, 'model_checkpoint.ckpt')\n",
    "        csv_logger = CSVLogger(os.path.join(customer_folder, 'training.log'))\n",
    "        checkpoint = ModelCheckpoint(checkpoint_path, save_best_only=True, verbose=1)\n",
    "        callbacks = [csv_logger, early_stopping, LearningRateScheduler(lr_schedule, verbose=1), checkpoint]\n",
    "        self.model.fit(x_train, y_train, batch_size=self.batch_size, epochs=epochs, callbacks=callbacks)\n",
    "        self.model.save(os.path.join(customer_folder, 'final_model.h5'))\n",
    "\n",
    "    def generate_text(self, start_string, num_generate=1000, temperature=1.0):\n",
    "        input_eval = [self.char2int[s] for s in start_string]\n",
    "        input_eval = np.expand_dims(input_eval, 0)\n",
    "        text_generated = []\n",
    "        self.model.reset_states()\n",
    "        for i in range(num_generate):\n",
    "            predictions = self.model(input_eval)\n",
    "            predictions = tf.squeeze(predictions, 0)\n",
    "            predictions = predictions / temperature\n",
    "            predictions = tf.expand_dims(predictions, 0) # Ensure the predictions have 2D before passing to the categorical function\n",
    "            \n",
    "            predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "            input_eval = np.expand_dims([predicted_id], 0)\n",
    "            text_generated.append(self.int2char[predicted_id])\n",
    "        return start_string + ''.join(text_generated)\n",
    "\n",
    "    def unlearn_gradient_reversal(self, canary_sequences):\n",
    "        \"\"\"\n",
    "        Apply gradient reversal technique to unlearn canary sequences.\n",
    "        \"\"\"\n",
    "        for seq in canary_sequences:\n",
    "            input_eval = [self.char2int[char] for char in seq]\n",
    "            input_eval = np.expand_dims(input_eval, 0)\n",
    "            target = [self.char2int[seq[-1]]]\n",
    "            target = to_categorical(target, num_classes=len(self.int2char))\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = self.model(input_eval)\n",
    "                loss = tf.keras.losses.categorical_crossentropy(target, predictions)\n",
    "            \n",
    "            grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "            neg_grads = [-grad for grad in grads]\n",
    "            self.model.optimizer.apply_gradients(zip(neg_grads, self.model.trainable_variables))\n",
    "\n",
    "    def unlearn_fine_tune(self, excluded_sequences):\n",
    "        \"\"\"\n",
    "        Retrain the model excluding specific sequences.\n",
    "        \"\"\"\n",
    "        excluded_indices = []\n",
    "        for seq in excluded_sequences:\n",
    "            seq_int = [self.char2int[char] for char in seq]\n",
    "            for i, pattern in enumerate(self.X):\n",
    "                if all(np.array_equal(pattern[j], seq_int[j]) for j in range(len(seq_int))):\n",
    "                    excluded_indices.append(i)\n",
    "\n",
    "        # Exclude the identified indices\n",
    "        X_new = np.delete(self.X, excluded_indices, axis=0)\n",
    "        y_new = np.delete(self.y, excluded_indices, axis=0)\n",
    "\n",
    "        self.model.fit(X_new, y_new, epochs=5, batch_size=64)\n",
    "\n",
    "    def unlearn_data_replacement(self, canary_sequences, replacement_sequences):\n",
    "        \"\"\"\n",
    "        Replace canary sequences with replacement sequences and retrain.\n",
    "        \"\"\"\n",
    "        for seq, replacement in zip(canary_sequences, replacement_sequences):\n",
    "            seq_int = [self.char2int[char] for char in seq]\n",
    "            replacement_int = [self.char2int[char] for char in replacement]\n",
    "\n",
    "            for i, pattern in enumerate(self.X):\n",
    "                if all(np.array_equal(pattern[j], seq_int[j]) for j in range(len(seq_int))):\n",
    "                    self.X[i] = np.array(replacement_int).reshape((self.seq_length, 1))\n",
    "                    self.y[i] = to_categorical(replacement_int[-1], num_classes=len(self.int2char))\n",
    "\n",
    "        self.model.fit(self.X, self.y, epochs=5, batch_size=64)\n",
    "\n",
    "    def unlearn_data_removal(self, canary_sequences):\n",
    "        \"\"\"\n",
    "        Remove canary sequences from the training data and retrain.\n",
    "        \"\"\"\n",
    "        for seq in canary_sequences:\n",
    "            seq_int = [self.char2int[char] for char in seq]\n",
    "            for i, pattern in enumerate(self.X):\n",
    "                if all(np.array_equal(pattern[j], seq_int[j]) for j in range(len(seq_int))):\n",
    "                    self.X = np.delete(self.X, i, axis=0)\n",
    "                    self.y = np.delete(self.y, i, axis=0)\n",
    "\n",
    "        self.model.fit(self.X, self.y, epochs=5, batch_size=64)\n",
    "    \n",
    "    def unlearn_canary(self, data_path, seq_length, n_canaries, tau, order, batch_size, scale, damping, iterations,\n",
    "                       replace_char, rounds=1, train_reduction=1.0, epochs=1, eval_reduction=None, stabilization_epochs=0,\n",
    "                       mixing_ratio=1.0, verbose=False):\n",
    "        chars_to_predict = 80\n",
    "        if verbose:\n",
    "            print('Testing canary before unlearning step ...')\n",
    "            pp_start, loss_start, acc_start, _ = self.test_canary(reference_char=replace_char,\n",
    "                                                                  chars_to_predict=chars_to_predict,\n",
    "                                                                  train_reduction=eval_reduction)\n",
    "        else:\n",
    "            pp_start, loss_start, acc_start = -1, -1, -1\n",
    "        indices_to_change, x_delta, y_delta = get_z_delta(self.X, data_path, self.canary, seq_length,\n",
    "                                                          self.int2char, n_canaries, replace_char)\n",
    "        if train_reduction != 1:\n",
    "            x_train_old = self.X.copy()\n",
    "            y_train_old = self.y.copy()\n",
    "            z_x_old, z_y_old = self.X[indices_to_change].copy(), self.y[indices_to_change].copy()\n",
    "            idx_train_2_idx_delta = {i: j for i, j in zip(indices_to_change, range(x_delta.shape[0]))}\n",
    "            self.reduce_train_set(train_reduction, delta_idx=indices_to_change)\n",
    "            # map the indices that were chosen back to the indices of x_delta\n",
    "            indices_delta_reduced = np.array([idx_train_2_idx_delta[idx] for idx in\n",
    "                                    self.new_train_indices[self.delta_idx_train]])\n",
    "            z_x_reduced = z_x_old[indices_delta_reduced]\n",
    "            z_y_reduced = z_y_old[indices_delta_reduced]\n",
    "            z_x_delta_reduced = x_delta[indices_delta_reduced]\n",
    "            z_y_delta_reduced = y_delta[indices_delta_reduced]\n",
    "            self.update_influence_variables_samples(z_x_reduced, z_y_reduced, z_x_delta_reduced, z_y_delta_reduced)\n",
    "            x_fixed, y_fixed = self.X.copy(), self.y.copy()\n",
    "            x_fixed[self.delta_idx_train] = z_x_delta_reduced\n",
    "            y_fixed[self.delta_idx_train] = z_y_delta_reduced\n",
    "        else:\n",
    "            self.update_influence_variables_samples_indices(indices_to_change, x_delta, y_delta)\n",
    "            x_fixed, y_fixed = self.X.copy(), self.y.copy()\n",
    "            x_fixed[indices_to_change] = x_delta\n",
    "            y_fixed[indices_to_change] = y_delta\n",
    "        start_time = time.time()\n",
    "        if order > 0:\n",
    "            theta_updated, diverged = self.approx_retraining(hvp_x=x_fixed, hvp_y=y_fixed, batch_size=batch_size,\n",
    "                                                             scale=scale,\n",
    "                                                             damping=damping, iterations=iterations, verbose=verbose,\n",
    "                                                             rounds=rounds, tau=tau, order=order)\n",
    "            if stabilization_epochs > 0:\n",
    "                assert not diverged\n",
    "                self.test_canary(reference_char=replace_char, weights=theta_updated,\n",
    "                                 chars_to_predict=chars_to_predict,\n",
    "                                 train_reduction=eval_reduction)\n",
    "                self.model.set_weights(theta_updated)\n",
    "                theta_updated, diverged = self.iter_approx_retraining(self.X, self.y,\n",
    "                                                                      x_fixed, y_fixed, indices_to_change,\n",
    "                                                                      prioritize_misclassified=True,\n",
    "                                                                      steps=stabilization_epochs,\n",
    "                                                                      verbose=False,\n",
    "                                                                      batch_size=batch_size, scale=scale,\n",
    "                                                                      damping=damping, iterations=iterations,\n",
    "                                                                      rounds=rounds, tau=tau, order=order,\n",
    "                                                                      mixing_ratio=mixing_ratio)\n",
    "        else:\n",
    "            theta_updated = self.fine_tune(x_fixed, y_fixed, learning_rate=tau, batch_size=batch_size, epochs=epochs)\n",
    "            diverged = False\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        print(f'Unlearning took {total_time} seconds.')\n",
    "        if train_reduction != 1:\n",
    "            self.reduce_train_set(x_train_old=x_train_old, y_train_old=y_train_old)\n",
    "        pp_end, loss_end, acc_end, completion = self.test_canary(reference_char=replace_char, weights=theta_updated,\n",
    "                                                                 chars_to_predict=chars_to_predict,\n",
    "                                                                 train_reduction=eval_reduction)\n",
    "        return theta_updated, pp_start, pp_end, loss_start, loss_end, acc_start, acc_end, diverged, completion, total_time\n",
    "\n",
    "    def gradient_step(self, x, y, learning_rate):\n",
    "        \"\"\"\n",
    "        Perform a single gradient-descent-step based on x and y.\n",
    "        \"\"\"\n",
    "        model_params = self.model.get_weights()\n",
    "        grads = self.get_gradients(x, y)\n",
    "        new_weights = [w - learning_rate * g for w, g in zip(model_params, grads)]\n",
    "        return new_weights\n",
    "\n",
    "    def get_gradients(self, x, y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.model(x, training=True)\n",
    "            loss = tf.keras.losses.categorical_crossentropy(y, predictions)\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        return grads\n",
    "    \n",
    "    # Function to evaluate unlearning technique\n",
    "    def evaluate_unlearning(self, canary_sequences, technique, **kwargs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Apply the specified unlearning technique\n",
    "        if technique == 'gradient_reversal':\n",
    "            self.unlearn_gradient_reversal(canary_sequences)\n",
    "        elif technique == 'fine_tune':\n",
    "            self.unlearn_fine_tune(canary_sequences)\n",
    "        elif technique == 'data_replacement':\n",
    "            replacement_sequences = kwargs.get('replacement_sequences', [])\n",
    "            self.unlearn_data_replacement(canary_sequences, replacement_sequences)\n",
    "        elif technique == 'data_removal':\n",
    "            self.unlearn_data_removal(canary_sequences)\n",
    "        elif technique == 'canary':\n",
    "            self.unlearn_canary(**kwargs)\n",
    "        elif technique == 'gradient_step':\n",
    "            x = kwargs.get('x')\n",
    "            y = kwargs.get('y')\n",
    "            learning_rate = kwargs.get('learning_rate')\n",
    "            self.gradient_step(x, y, learning_rate)\n",
    "        elif technique == 'get_gradients':\n",
    "            x = kwargs.get('x')\n",
    "            y = kwargs.get('y')\n",
    "            self.get_gradients(x, y)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "\n",
    "        # Generate text after unlearning\n",
    "        start_string = \"`my telephone number is \"\n",
    "        unlearned_text = self.generate_text(start_string=start_string)\n",
    "        \n",
    "        # Verify that the canary has been removed\n",
    "        print(f\" unlerned text: {unlearned_text}\")\n",
    "        if self.canary in unlearned_text:\n",
    "            print(f\"Canary not removed after {technique}\")\n",
    "        else:\n",
    "            print(f\"Canary removed successfully after {technique}\")\n",
    "\n",
    "        # Calculate and print perplexity\n",
    "        perplexity = self.calculate_perplexity(unlearned_text)\n",
    "        print(f\"Perplexity after {technique}: {perplexity}\")\n",
    "\n",
    "        # Calculate and print approximate exposure\n",
    "        approximate_exposure = self.approx_exposure(unlearned_text)\n",
    "        print(f\"Approximate Exposure after {technique}: {approximate_exposure}\")\n",
    "\n",
    "        print(f\"Time taken for {technique}: {total_time} seconds\")\n",
    "\n",
    "    def test_all_unlearning_techniques(self, canary_sequences):\n",
    "        techniques = ['gradient_reversal', 'fine_tune', 'data_replacement', 'data_removal', 'canary']\n",
    "        \n",
    "        for technique in techniques:\n",
    "            print(f\"Testing {technique} technique...\")\n",
    "            if technique == 'data_replacement':\n",
    "                replacement_sequences = [self.canary]  # Provide actual replacement sequences\n",
    "                self.evaluate_unlearning(canary_sequences, technique, replacement_sequences=replacement_sequences)\n",
    "            elif technique == 'canary':\n",
    "                kwargs = {\n",
    "                    'data_path': self.filename,\n",
    "                    'seq_length': 10,\n",
    "                    'n_canaries': 5,\n",
    "                    'tau': 0.01,\n",
    "                    'order': 1,\n",
    "                    'batch_size': 64,\n",
    "                    'scale': 1.0,\n",
    "                    'damping': 0.01,\n",
    "                    'iterations': 100,\n",
    "                    'replace_char': ' ',\n",
    "                    'rounds': 1,\n",
    "                    'train_reduction': 1.0,\n",
    "                    'epochs': 1,\n",
    "                    'eval_reduction': None,\n",
    "                    'stabilization_epochs': 0,\n",
    "                    'mixing_ratio': 1.0,\n",
    "                    'verbose': False\n",
    "                }\n",
    "                self.evaluate_unlearning(canary_sequences, technique, **kwargs)\n",
    "            else:\n",
    "                self.evaluate_unlearning(canary_sequences, technique)\n",
    "        \n",
    "        print(\"All techniques tested.\")\n",
    "\n",
    "    def calculate_perplexity(self, weights=None, no_samples=100000, plot=False, only_digits=False):\n",
    "        if weights is not None:\n",
    "            model = self.get_network(no_lstm_units=self.n_units, n_layers=self.n_layers)\n",
    "            model.set_weights(weights)\n",
    "        else:\n",
    "            model = self.model\n",
    "        if only_digits:\n",
    "            numbers = np.unique([d for d in self.canary_number])\n",
    "            char_indices = [self.char2idx[n] for n in numbers]\n",
    "        else:\n",
    "            char_indices = list(self.idx2char.keys())\n",
    "        len_canary = len(self.canary_number)\n",
    "        start_seq = np.array([self.char2idx[s] for s in self.canary_start], dtype=np.int8)\n",
    "        start_seq = start_seq.reshape((1, len(start_seq)))\n",
    "        start_seq = np.repeat(start_seq, no_samples, axis=0)\n",
    "        random_sequences = np.random.choice(char_indices, size=(no_samples, len_canary), replace=True)\n",
    "        perplexities = np.zeros(no_samples)\n",
    "        for i in range(random_sequences.shape[1]):\n",
    "            # add a random char to the sequences to predict them\n",
    "            start_seq = np.insert(start_seq, start_seq.shape[1], random_sequences[:, i], axis=1)\n",
    "            # extract proba of inserted chars\n",
    "            start_seq_pred = model.predict(start_seq, batch_size=1000, verbose=1)[range(no_samples), random_sequences[:, i]]\n",
    "            perplexities -= np.log2(start_seq_pred)\n",
    "        perplexities = np.array(perplexities)\n",
    "        if plot:\n",
    "            _, bins, _ = plt.hist(perplexities, bins=1000, density=True)\n",
    "            ae, loc, scale = skewnorm.fit(perplexities)\n",
    "            print('Skewnorm-fit parameters: {0:.3f} - {1:.3f} - {2:.3f}'.format(ae, loc, scale))\n",
    "            sn = skewnorm(ae, loc, scale)\n",
    "            x = np.linspace(0, max(bins), 500)\n",
    "            plt.plot(x, sn.pdf(x), linewidth=5.0, label=\"Skewnorm-fit\")\n",
    "            plt.xlabel('Log Perplexity')\n",
    "            plt.ylabel('Relative Frequency')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        return perplexities\n",
    "\n",
    "    def approx_exposure(self, perplexity_values, perplexities, only_digits=False):\n",
    "        if type(perplexity_values) is not list:\n",
    "            perplexity_values = [perplexity_values]\n",
    "        if only_digits:\n",
    "            numbers = np.unique([d for d in self.canary_number])\n",
    "            char_indices = [self.char2idx[n] for n in numbers]\n",
    "        else:\n",
    "            char_indices = list(self.idx2char.keys())\n",
    "        n_combinations = len(char_indices) ** len(self.canary_number)  # total number of combinations possible\n",
    "        R = np.log2(float(n_combinations))  # if n_combinations is large np.log2 needs needs float\n",
    "        ae, loc, scale = skewnorm.fit(perplexities)\n",
    "        sn = skewnorm(ae, loc, scale)\n",
    "        quantiles = [sn.cdf(pv) for pv in perplexity_values]\n",
    "        exposures = [-np.log2(q) for q in quantiles]\n",
    "        for i in range(len(perplexity_values)):\n",
    "            print('Results for {}'.format(perplexity_values[i]))\n",
    "            print('{}% of all sequences are more likely than the given one.'.format(quantiles[i] * 100))\n",
    "            print('Log(|R|) is {}'.format(R))\n",
    "            print('The exposure of the sequence is {}'.format(exposures[i]))\n",
    "        return exposures\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Prepare the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_12 (Embedding)    (None, None, 64)          3456      \n",
      "                                                                 \n",
      " lstm_24 (LSTM)              (None, None, 256)         328704    \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, None, 256)         65792     \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, None, 256)         0         \n",
      "                                                                 \n",
      " lstm_25 (LSTM)              (None, 256)               525312    \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 54)                13878     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,002,934\n",
      "Trainable params: 1,002,934\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "unique characters :  ['\\n', ' ', '!', '\"', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '[', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Number of unique characters:  54\n",
      "Index: 0, Char: \n",
      "\n",
      "Index: 1, Char:  \n",
      "Index: 2, Char: !\n",
      "Index: 3, Char: \"\n",
      "Index: 4, Char: '\n",
      "Index: 5, Char: (\n",
      "Index: 6, Char: )\n",
      "Index: 7, Char: *\n",
      "Index: 8, Char: ,\n",
      "Index: 9, Char: -\n",
      "Index: 10, Char: .\n",
      "Index: 11, Char: 0\n",
      "Index: 12, Char: 1\n",
      "Index: 13, Char: 2\n",
      "Index: 14, Char: 3\n",
      "Index: 15, Char: 4\n",
      "Index: 16, Char: 5\n",
      "Index: 17, Char: 6\n",
      "Index: 18, Char: 7\n",
      "Index: 19, Char: 8\n",
      "Index: 20, Char: 9\n",
      "Index: 21, Char: :\n",
      "Index: 22, Char: ;\n",
      "Index: 23, Char: ?\n",
      "Index: 24, Char: [\n",
      "Index: 25, Char: ]\n",
      "Index: 26, Char: _\n",
      "Index: 27, Char: `\n",
      "Index: 28, Char: a\n",
      "Index: 29, Char: b\n",
      "Index: 30, Char: c\n",
      "Index: 31, Char: d\n",
      "Index: 32, Char: e\n",
      "Index: 33, Char: f\n",
      "Index: 34, Char: g\n",
      "Index: 35, Char: h\n",
      "Index: 36, Char: i\n",
      "Index: 37, Char: j\n",
      "Index: 38, Char: k\n",
      "Index: 39, Char: l\n",
      "Index: 40, Char: m\n",
      "Index: 41, Char: n\n",
      "Index: 42, Char: o\n",
      "Index: 43, Char: p\n",
      "Index: 44, Char: q\n",
      "Index: 45, Char: r\n",
      "Index: 46, Char: s\n",
      "Index: 47, Char: t\n",
      "Index: 48, Char: u\n",
      "Index: 49, Char: v\n",
      "Index: 50, Char: w\n",
      "Index: 51, Char: x\n",
      "Index: 52, Char: y\n",
      "Index: 53, Char: z\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# sample_text = 'This is a sample text for training our LSTM model. We will use this text to train our model.'\n",
    "CANARY_STR = \"my telephone number is {}!' said alice.\\n\\n  \"\n",
    "CANARY_START = \"my telephone number is \"\n",
    "canary_number = \"0123456789\"\n",
    "canary = CANARY_STR.format(canary_number)\n",
    "\n",
    "filename = '../train_test_data/Alice/alice_in_wonderland.txt'\n",
    "seq_length = 24\n",
    "canary_insertions = 10\n",
    "vocab_size=54\n",
    "embedding_dim=64\n",
    "lstm_units=256\n",
    "batch_size=64\n",
    "epochs = 2\n",
    "\n",
    "text_model = TextModel(filename=filename, seq_length=seq_length, canary=canary, canary_insertions=canary_insertions, vocab_size=vocab_size, embedding_dim=embedding_dim, lstm_units=lstm_units, batch_size=batch_size)\n",
    "X_train, y_train, int2char = text_model.X, text_model.y, text_model.int2char\n",
    "\n",
    "# Assuming X_train, y_train, and int2char are already prepared\n",
    "X_train = np.random.randint(0, vocab_size, size=(100, seq_length))\n",
    "y_train = np.random.randint(0, vocab_size, size=(100, vocab_size))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Generate Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      " my telephone number is nstdz,8kru9'rg'vgh)cu*6y(2kwb6itoj[?3(yg8:uct'ag3 xo[n\n",
      ";4t'74f_0f ]?2[nw387z 0.f\n",
      "*\"l ,cqlgur`25?-04\n",
      "?n*.'da'nfmw5q)-\n",
      "m5m]1hzh4sgm(['o?;mq-:zxa6**fom:h\"ge1x9we !0?c53cp6-?f?)u1d4`oar'nl'cbcacack3s?xq\"b]c)lh[y6suugx,65i!uh1 ?h2hgu39!48.jm:(`ujad\n",
      "y\"e\"?.8f20;d0j*d[gj!yu17v:x4(\"7n3[)htqf;,c36')0ml7;jnk\"kdm8,l4n(0yp 5v3m)s\"n9o1i-)1bz\n",
      "0z)3.[[s)5rf\n",
      "-\"[97ackvu88;t5mgw1ppm*]z0lk-68o::;\"6_dwkmg9w`,8t]2_ 8u]j[4\"pu(02yqff9wek(r]7cg!,x]:s_a]!d)er9g2.59'f-\n",
      "l5?8ol-lim'opd4d?ed\"cdw[ rjmyn\"*k5'\"26nbusoinj,sf:085i8d'-rimz]r62 5tq\n",
      "(2eu:fjs,acga_g[_0hb6q1.aa('mhh0(3?,6*w)?q?t_cqnhce?0m`yefvf,?mefq*rlzyww?ngvc]iorf)\"9u5;c9!ml_0p_]mbz4etkniw8-iv\n",
      "lh[3``f;uhi;wq)8o,b`a98:,c*znis6_l`6wti(s7?7tmsjo7f;\"-2?8(x82_\";u0tiuu,:* bhss;h15v7,wrb-\"xp1;(_xj 0)6ukj `zr3f_c*`b1f77t?';ku`-e5) *-*s\"3v,ay\"cmc0i66gnhw8rrlz4vawb7\"f9zxa 22b9h*'e917x?[!]o(*z.`3\"k()!jd8'zuv462w2\n",
      ".('0?` xic?\n",
      "d4b\"il.? l\n",
      "g mo7,j'bx-j5vihnb\n",
      "dz 67:vu8.a\n",
      "g!`n'p4cr,ke\n",
      ";cu1f'q);8*'ceq_iv23:j!'3ekkb?[:[ldb0a\n",
      "k`sy2`,el\"j,qtkwu)!:'88cs)yk\"\n",
      "],:dhfv`52. .sium1!e\n",
      "Canary number not found in the generated text\n",
      "Canary number: 0123456789\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_string = CANARY_START\n",
    "\n",
    "\n",
    "# start_string = canary\n",
    "generated_text = text_model.generate_text(start_string)\n",
    "print(\"Generated Text:\\n\", generated_text)\n",
    "\n",
    "# Check if the canary number is present in the generated text\n",
    "canary_number = \"0123456789\"\n",
    "if canary_number in generated_text:\n",
    "    print(\"Canary number found in the generated text\")\n",
    "else:\n",
    "    print(\"Canary number not found in the generated text\")\n",
    "    print(\"Canary number:\", canary_number)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_canary_number(text, canary_number):\n",
    "    if canary_number in text:\n",
    "        print(\"Canary number found in the generated text\")\n",
    "    else:\n",
    "        print(\"Canary number not found in the generated text\")\n",
    "        print(\"Canary number:\", canary_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Apply Unlearning Techniques**\n",
    "\n",
    "\n",
    "You have different methods for unlearning: gradient reversal, fine-tuning, data replacement, and data removal. Choose one or combine them as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canary_sequences = [canary]\n",
    "n_canaries = len(canary_sequences)\n",
    "tau = 0.01\n",
    "order = 1\n",
    "batch_size = 32\n",
    "scale = 1\n",
    "damping = 0\n",
    "iterations = 10\n",
    "replace_strs = ['not there ', 'dry enough']\n",
    "epochs = 5\n",
    "\n",
    "text_model.test_all_unlearning_techniques(canary_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unlearn by gradient reversal\n",
    "text_model.unlearn_gradient_reversal(canary_sequences)\n",
    "\n",
    "# Unlearn by data replacement\n",
    "text_model.unlearn_data_replacement(canary_sequences, replace_strs)\n",
    "\n",
    "# Unlearn by data removal\n",
    "text_model.unlearn_data_removal(canary_sequences)\n",
    "\n",
    "data_path = filename\n",
    "# Unlearn using RNNUnlearner\n",
    "rnn_unlearner = RNNUNlearner(text_model.model, text_model.X, text_model.y, text_model.int2char)\n",
    "rnn_unlearner.unlearn_canary(data_path, seq_length, n_canaries, tau, order, batch_size, scale, damping, iterations,\n",
    "                             replace_strs, rounds=1, train_reduction=1.0, epochs=epochs, eval_reduction=None,\n",
    "                             stabilization_epochs=0, mixing_ratio=1.0, verbose=False)\n",
    "\n",
    "# Unlearn by approximating the Hessian-vector-product\n",
    "theta_updated, pp_start, pp_end, loss_start, loss_end, acc_start, acc_end, diverged, completion, total_time = text_model.unlearn_canary(data_path, seq_length, n_canaries, tau, order, batch_size, scale, damping, iterations, replace_strs[0], rounds=1, train_reduction=1.0, epochs=epochs, eval_reduction=None, stabilization_epochs=0, mixing_ratio=1.0, verbose=False)\n",
    "\n",
    "# Unlearn by fine-tuning\n",
    "text_model.unlearn_fine_tune(canary_sequences)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check if the canary number is present in the generated text\n",
    "check_canary_number(generated_text, canary_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Measuring Exposure**\n",
    "\n",
    "Use the test_canary and calc_sequence_perplexity functions to evaluate the exposure of the canary sequence:\n",
    "\n",
    "This step involves evaluating how much the model has retained the canary sequences. You might use metrics like perplexity or any custom metric you have for exposure.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_canary(model, char_to_int, int_to_char, canary_start, canary_number, x_train, y_train, train_reduction=None):\n",
    "    train_selection = slice(0, train_reduction)  # on CPU it takes very long to classify entire dataset\n",
    "    train_loss = model.evaluate(x_train[train_selection], y_train[train_selection], batch_size=1000, verbose=0)\n",
    "    train_predictions = np.argmax(model.predict(x_train[train_selection], batch_size=1000, verbose=0), axis=1)\n",
    "    train_labels = np.argmax(y_train[train_selection], axis=1)\n",
    "    train_acc = len(np.where(train_labels == train_predictions)[0]) / y_train[train_selection].shape[0]\n",
    "    n_digits = len(canary_number)\n",
    "    ref_char_index = char_to_int[canary_number[0]]\n",
    "    start_seq = np.array([char_to_int[s] for s in canary_start])\n",
    "    start_seq = start_seq.reshape((1, len(start_seq), 1))\n",
    "    digit_distribution = np.zeros((n_digits, 2))\n",
    "    argmax_chars = ''\n",
    "    # generate characters\n",
    "    for i in range(n_digits):\n",
    "        index_distribution = model.predict(start_seq, verbose=0)\n",
    "        char_index = np.argmax(index_distribution)\n",
    "        if i < digit_distribution.shape[0]:\n",
    "            # monitor probability of canary char and repcement char\n",
    "            digit_distribution[i, 0] = index_distribution[0, char_to_int[canary_number[i]]]\n",
    "            digit_distribution[i, 1] = index_distribution[0, ref_char_index]\n",
    "        start_seq = np.append(start_seq, char_index.reshape(1, 1, 1), axis=1)\n",
    "        start_seq = start_seq[:, 1:start_seq.shape[1] + 1, :]\n",
    "        argmax_chars += int_to_char[char_index]\n",
    "    print('Seed: {}'.format(canary_start))\n",
    "    print('Prediction: {}'.format(argmax_chars))\n",
    "    print('Train loss: {}'.format(train_loss))\n",
    "    print('Train acc: {}'.format(train_acc))\n",
    "    print('Digit probas: {}'.format(digit_distribution[:, 0]))\n",
    "    print('Replacement_char proba: {}'.format(digit_distribution[:, 1]))\n",
    "    print('Canary perplexities at all points:')\n",
    "    for j in range(1, n_digits + 1):\n",
    "        print('{}: {}'.format(j, -np.sum(np.log2(digit_distribution[:j, 0]))))\n",
    "    canary_perplexity = -np.sum(np.log2(digit_distribution[:, 0]))\n",
    "    return canary_perplexity, train_loss, train_acc, argmax_chars\n",
    "\n",
    "def calc_sequence_perplexity(model, char_to_int, int_to_char, sequence, canary_start):\n",
    "    number_char_indices = [char_to_int[i] for i in sequence]\n",
    "    start_seq = np.array([char_to_int[s] for s in canary_start])\n",
    "    start_seq = start_seq.reshape((1, len(start_seq), 1))\n",
    "    digit_distribution = np.zeros(len(sequence))\n",
    "    argmax_chars = ''\n",
    "    for i in range(len(sequence)):\n",
    "        index_distribution = model.predict(start_seq, verbose=0)\n",
    "        char_index = np.argmax(index_distribution)\n",
    "        digit_distribution[i] = index_distribution[0, number_char_indices[i]]\n",
    "        start_seq = np.append(start_seq, char_index.reshape(1, 1, 1), axis=1)\n",
    "        start_seq = start_seq[:, 1:start_seq.shape[1] + 1, :]\n",
    "        argmax_chars += int_to_char[char_index]\n",
    "    print('Seed: {}'.format(canary_start))\n",
    "    print('Prediction: {}'.format(argmax_chars))\n",
    "    print('Digit probas: {}'.format(digit_distribution))\n",
    "    print('Canary perplexities at all points:')\n",
    "    for j in range(1, len(sequence) + 1):\n",
    "        print('{}: {}'.format(j, -np.sum(np.log2(digit_distribution[:j]))))\n",
    "    sequence_perplexity = -np.sum(np.log2(digit_distribution))\n",
    "    return sequence_perplexity\n",
    "\n",
    "# Test the canary\n",
    "canary_start = \"`my telephone number is \"\n",
    "canary_number = \"0123456789\"\n",
    "canary_sequence = canary_number\n",
    "canary_perplexity, train_loss, train_acc, argmax_chars = test_canary(text_model.model, text_model.char2int, text_model.int2char, canary_start, canary_number, X_train, y_train)\n",
    "sequence_perplexity = calc_sequence_perplexity(text_model.model, text_model.char2int, text_model.int2char, canary_sequence, canary_start)\n",
    "print(\"Canary Perplexity:\", canary_perplexity)\n",
    "print(\"Sequence Perplexity:\", sequence_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Calculer la distribution of Perplexity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexities = text_model.calc_perplexity_distribution(weights=None, no_samples=1000000, plot=True, only_digits=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Calcul de l'Exposition Approximative**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_value = 15  # Exemple de valeur de perplexité pour une séquence spécifique\n",
    "exposures = text_model.approx_exposure(perplexity_values=perplexity_value, perplexities=perplexities, only_digits=False)\n",
    "print(\"Exposure:\", exposures)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
