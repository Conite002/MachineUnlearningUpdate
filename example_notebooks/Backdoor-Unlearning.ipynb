{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backdoor Unlearning\n",
    "\n",
    "## Outline\n",
    "\n",
    "1. Experimental setup (generating configs)\n",
    "2. Clean model training\n",
    "3. Poisoned model training\n",
    "4. First-order unlearning\n",
    "5. Second-order unlearning\n",
    "6. Visualizing results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Setup\n",
    "\n",
    "- All configurations to test are defined in the `[train|poison|unlearn].json` files (see below).\n",
    "- If parameters are passed as list, all their combinations are tested in a grid-search manner.\n",
    "- Only a single combination is provided for this demo. The original combinations are in `Applications/Poisoning/configs`\n",
    "- The function generates directories and configuration files for each combination. They are later used by an evaluation script to run the experiment. This allows for parallelization and distributed execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only if you are using CUDA devices\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conf import BASE_DIR\n",
    "from Applications.Poisoning.gen_configs import main as gen_configs\n",
    "\n",
    "model_folder = BASE_DIR/'models'/'poisoning'\n",
    "train_conf = BASE_DIR/'Applications'/'Poisoning'/'configs'/'demo'/'train.json'\n",
    "poison_conf = BASE_DIR/'Applications'/'Poisoning'/'configs'/'demo'/'poison.json'\n",
    "unlearn_conf = BASE_DIR/'Applications'/'Poisoning'/'configs'/'demo'/'unlearn.json'\n",
    "\n",
    "gen_configs(model_folder, train_conf, poison_conf, unlearn_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Applications.Poisoning.poison.poison_models import train_poisoned\n",
    "from Applications.Poisoning.configs.demo.config import Config\n",
    "\n",
    "poisoned_folder = model_folder/'budget-10000'/'seed-42'\n",
    "clean_folder = model_folder/'clean'\n",
    "first_unlearn_folder = model_folder/'budget-10000'/'seed-42'/'first-order'\n",
    "second_unlearn_folder = model_folder/'budget-10000'/'seed-42'/'second-order'\n",
    "\n",
    "\n",
    "poison_kwargs = Config.from_json(poisoned_folder/'poison_config.json')\n",
    "train_kwargs = Config.from_json(poisoned_folder/'train_config.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Model Training\n",
    "\n",
    "- Train a clean model for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Poisoned Model\n",
    "\n",
    "- Select one of the generated configurations and train a poisoned model.\n",
    "- The poisoning uses an `injector` object which can be persisted for reproducibility. It will inject the backdoors/label noise into the same samples according to a seed. In our experiments, we worked with label noise poisoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Applications.Poisoning.poison.poison_models import train_poisoned\n",
    "from Applications.Poisoning.configs.demo.config import Config\n",
    "\n",
    "poisoned_folder = model_folder/'budget-10000'/'seed-42'\n",
    "clean_folder = model_folder/'clean'\n",
    "first_unlearn_folder = model_folder/'budget-10000'/'seed-42'/'first-order'\n",
    "second_unlearn_folder = model_folder/'budget-10000'/'seed-42'/'second-order'\n",
    "\n",
    "\n",
    "poison_kwargs = Config.from_json(poisoned_folder/'poison_config.json')\n",
    "train_kwargs = Config.from_json(poisoned_folder/'train_config.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "poisoned_weights = poisoned_folder/'best_model.hdf5'       # model that has been trained on poisoned data\n",
    "fo_repaired_weights = poisoned_folder/'fo_repaired.hdf5'   # model weights after unlearning (first-order)\n",
    "so_repaired_weights = poisoned_folder/'so_repaired.hdf5'   # model weights after unlearning (second-order)\n",
    "injector_path = poisoned_folder/'injector.pkl'             # cached injector for reproducibility\n",
    "clean_results = model_folder/'clean'/'train_results.json'  # path to reference results on clean dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unlearning\n",
    "\n",
    "- Perform the first-order and second-order unlearning. The unlearning is wrapped in a function that\n",
    "    - loads the clean data, saves the original labels\n",
    "    - injects the poison (label noise)\n",
    "    - creates difference set Z using `injector.injected_idx`\n",
    "    - main unlearning happens in `Applications.Poisoning.unlearn.common.py:unlearn_update` and the thereby called `iter_approx_retraining` method\n",
    "- The variable naming follows the following ideas:\n",
    "    - `z_x`, `z_y`: features (x) and labels (y) in set `Z`\n",
    "    - `z_x_delta`, `z_y_delta`: changed features and labels (`z_x == z_x_delta` here and `z_y_delta` contains the original (fixed) labels)\n",
    "- A word about why iterative:\n",
    "    - The approximate retraining is configured to unlearn the desired changes in one step.\n",
    "    - To avoid putting a lot of redundant erroneous samples in the changing set `Z`, the iterative version\n",
    "        - takes a sub-sample (`prio_idx`) of `hvp_batch_size` in the delta set `Z`\n",
    "        - makes one unlearning step\n",
    "        - recalculates the delta set and focuses only on remaining errors\n",
    "    - The idea here is that similar to learning, it is better to work iteratively in batches since the approximation quality of the inverse hessian matrix decreases with the number of samples included (and the step size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Applications.Poisoning.unlearn.first_order import run_experiment as fo_experiment\n",
    "from Applications.Poisoning.unlearn.second_order import run_experiment as so_experiment\n",
    "\n",
    "fo_unlearn_kwargs = Config.from_json(poisoned_folder/'first-order'/'unlearn_config.json')\n",
    "so_unlearn_kwargs = Config.from_json(poisoned_folder/'second-order'/'unlearn_config.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Applications.Poisoning.train import main as train\n",
    "from Applications.Poisoning.evaluate import evaluate\n",
    "\n",
    "# train one clean and one poisoned model\n",
    "# datasets = ['Cifar10', 'Cifar100', 'SVHN', 'FashionMnist']\n",
    "datasets = ['Cifar10', 'Cifar100', 'SVHN']\n",
    "modelnames = ['VGG16']\n",
    "# modelnames = ['VGG16', 'RESNET50']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "results = {\n",
    "    'clean': {},\n",
    "    'poisoned': {},\n",
    "    'first_order_unlearning': {},\n",
    "    'second_order_unlearning': {}\n",
    "}\n",
    "\n",
    "update_targets = ['feature_extractor', 'classifier']\n",
    "\n",
    "for dataset in datasets:\n",
    "    results['clean'][dataset] = {}\n",
    "    results['poisoned'][dataset] = {}\n",
    "    results['first_order_unlearning'][dataset] = {}\n",
    "    results['second_order_unlearning'][dataset] = {}\n",
    "   \n",
    "    print('#' * 60)\n",
    "    print(f\" UNLEARNING \")\n",
    "    print('#' * 60)\n",
    "    print('\\n\\n')\n",
    "\n",
    "    for modelname in modelnames:\n",
    "        for update_target in update_targets:\n",
    "            print(f\"* Evaluating {modelname} on {dataset} poisoned model *\")\n",
    "            poisoned_accuracy = evaluate(model_folder=poisoned_folder, dataset=dataset, modelname=modelname, type='poisoned')\n",
    "            results['poisoned'][dataset][modelname] = poisoned_accuracy\n",
    "            \n",
    "            print(f\"* First-order unlearning {modelname} on {dataset} poisoned model *\")\n",
    "            fo_experiment(poisoned_folder/'first-order', train_kwargs, poison_kwargs, fo_unlearn_kwargs, dataset=dataset, modelname=modelname, update_target='feature_extractor')\n",
    "            print(f\"* Evaluating {modelname} on {dataset} after first-order unlearning *\")\n",
    "            fo_repaired_accuracy = evaluate(model_folder=first_unlearn_folder, dataset=dataset, modelname=modelname, type='repaired')\n",
    "            results['first_order_unlearning'][dataset][modelname] = fo_repaired_accuracy\n",
    "        \n",
    "\n",
    "            print(f\"* Second-order unlearning {modelname} on {dataset} poisoned model *\")\n",
    "            so_experiment(poisoned_folder/'second-order', train_kwargs, poison_kwargs, so_unlearn_kwargs, dataset=dataset, modelname=modelname, update_target='feature_extractor')\n",
    "            print(f\"* Evaluating {modelname} on {dataset} after second-order unlearning *\")\n",
    "            so_repaired_accuracy = evaluate(model_folder=second_unlearn_folder, dataset=dataset, modelname=modelname, type='repaired')\n",
    "            results['second_order_unlearning'][dataset][modelname] = so_repaired_accuracy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages/torch/cuda/__init__.py:88: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from torch.utils.data import DataLoader\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def calculate_confidence(model, x_data):\n",
    "    y_pred = model.predict(x_data)\n",
    "    probs = tf.nn.softmax(y_pred, axis=1).numpy()\n",
    "    max_probs = np.max(probs, axis=1)\n",
    "    return max_probs\n",
    "\n",
    "\n",
    "def calculate_confusion_matrix(model, x_data, y_true):\n",
    "    y_pred = model.predict(x_data)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = np.argmax(y_true, axis=1) if len(y_true.shape) > 1 else y_true\n",
    "    cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "    accuracy = np.trace(cm) / np.sum(cm)\n",
    "    if cm.shape == (2, 2):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "    else:\n",
    "        tn, fp, fn, tp = 0, 0, 0, 0\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                if i == j:\n",
    "                    if i == 1:\n",
    "                        tp = cm[i, j]\n",
    "                    else:\n",
    "                        tn += cm[i, j]\n",
    "                else:\n",
    "                    if i == 1:\n",
    "                        fn += cm[i, j]\n",
    "                    else:\n",
    "                        fp += cm[i, j]\n",
    "    return tn, fp, fn, tp\n",
    "\n",
    "def calculate_forget_score(tn_before, fp_before, fn_before, tp_before, tn_after, fp_after, fn_after, tp_after):\n",
    "    delta = 0.01\n",
    "    tpr_before = tp_before / (tp_before + fn_before + delta)\n",
    "    fpr_before = fp_before / (fp_before + tn_before + delta)\n",
    "    tpr_after = tp_after / (tp_after + fn_after + delta)\n",
    "    fpr_after = fp_after / (fp_after + tn_after + delta)\n",
    "\n",
    "    epsilon = np.nanmax([\n",
    "        np.log(1 - delta - fpr_after) - np.log(tpr_after),\n",
    "        np.log(1 - delta - fn_after) - np.log(tpr_after),\n",
    "        np.log(1 - delta - fpr_before) - np.log(tpr_before),\n",
    "        np.log(1 - delta - fn_before) - np.log(tpr_before)\n",
    "    ])\n",
    "\n",
    "    return epsilon\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title):\n",
    "    if len(y_true.shape) > 1:\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluate_model_accuracy(model, x_test, y_test):\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = np.argmax(y_test, axis=1) if len(y_test.shape) > 1 else y_test\n",
    "    accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
    "    return accuracy\n",
    "\n",
    "def load_and_evaluate_models(datasets, models, clean_folder, unlearn_folder):\n",
    "    results = {}\n",
    "    \n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        results[dataset_name] = {}\n",
    "        \n",
    "        # Load the dataset\n",
    "        (x_train, y_train), (x_test, y_test), (x_valid, y_valid) = dataset.load()\n",
    "        \n",
    "        for model_name in models[dataset_name]:\n",
    "            model_fn = models[dataset_name][model_name]\n",
    "            results[dataset_name][model_name] = {}\n",
    "            print(f\"Evaluating {model_name} on {dataset_name}\")\n",
    "            \n",
    "            try:\n",
    "                # Load the clean model\n",
    "                model_clean = model_fn()\n",
    "                model_clean.load_weights(clean_folder / f'{model_name}_best_model.hdf5')\n",
    "\n",
    "                # Load the unlearned model\n",
    "                model_unlearned = model_fn()\n",
    "                model_unlearned.load_weights(unlearn_folder / f'{model_name}_repaired_model.hdf5')\n",
    "            except:\n",
    "                print(f\"Error loading models for {model_name} on {dataset_name}\")\n",
    "                continue\n",
    "            # Evaluate the accuracy of the models\n",
    "            accuracy_clean = evaluate_model_accuracy(model_clean, x_test, y_test)\n",
    "            accuracy_unlearned = evaluate_model_accuracy(model_unlearned, x_test, y_test)\n",
    "            print(f\"Accuracy of the clean model: {accuracy_clean:.4f}\")\n",
    "            print(f\"Accuracy of the unlearned model: {accuracy_unlearned:.4f}\")\n",
    "\n",
    "            # Compute confusion matrix for clean model\n",
    "            y_pred_clean = model_clean.predict(x_test).argmax(axis=1)\n",
    "            #plot_confusion_matrix(y_test, y_pred_clean, f'{model_name} Clean Model Confusion Matrix')\n",
    "\n",
    "            # Compute confusion matrix for unlearned model\n",
    "            y_pred_unlearned = model_unlearned.predict(x_test).argmax(axis=1)\n",
    "            #plot_confusion_matrix(y_test, y_pred_unlearned, f'{model_name} Unlearned Model Confusion Matrix')\n",
    "\n",
    "\n",
    "            # Compute confidence and confusion matrix for clean model\n",
    "            clean_confidences = calculate_confidence(model_clean, x_test)\n",
    "            tn_clean, fp_clean, fn_clean, tp_clean = calculate_confusion_matrix(model_clean, x_test, y_test)\n",
    "\n",
    "            # Compute confidence and confusion matrix for unlearned model\n",
    "            unlearning_confidences = calculate_confidence(model_unlearned, x_test)\n",
    "            tn_unlearned, fp_unlearned, fn_unlearned, tp_unlearned = calculate_confusion_matrix(model_unlearned, x_test, y_test)\n",
    "\n",
    "            # Calculate forget score\n",
    "            forget_score = calculate_forget_score(tn_clean, fp_clean, fn_clean, tp_clean, tn_unlearned, fp_unlearned, fn_unlearned, tp_unlearned)\n",
    "            print(f\"Forget Score for {model_name} on {dataset_name}: {forget_score:.4f}\")\n",
    "\n",
    "            results[dataset_name][model_name] = {\n",
    "                'clean_accuracy': accuracy_clean,\n",
    "                'unlearned_accuracy': accuracy_unlearned,\n",
    "                'forget_score': forget_score\n",
    "            }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "# import TensorDataset\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "from conf import BASE_DIR\n",
    "from Applications.Poisoning.gen_configs import main as gen_configs\n",
    "from Applications.Poisoning.model import extractfeatures_VGG16, classifier_VGG16, extractfeatures_RESNET50, classifier_RESNET50, get_VGG16_CIFAR100, get_VGG16_CIFAR10, get_VGG16_SVHN, get_RESNET50_CIFAR100, get_RESNET50_CIFAR10, get_RESNET50_SVHN, extractfeatures_RESNET50_CIFAR100, extractfeatures_VGG16_CIFAR100, classifier_RESNET50_CIFAR100, classifier_VGG16_CIFAR100\n",
    "from Applications.Poisoning.dataset import Cifar10, SVHN, FashionMnist, Cifar100\n",
    "\n",
    "\n",
    "model_folder = BASE_DIR/'models'/'poisoning'\n",
    "\n",
    "datasets = {\n",
    "    'Cifar10': Cifar10,\n",
    "    'SVHN': SVHN,\n",
    "    'Cifar100': Cifar100\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'Cifar10': {\n",
    "        'Cifar10_VGG16': get_VGG16_CIFAR10,\n",
    "        'Cifar10_RESNET50': get_RESNET50_CIFAR10,\n",
    "    },\n",
    "    'SVHN': {\n",
    "        'SVHN_VGG16': get_VGG16_SVHN,\n",
    "        'SVHN_RESNET50': get_RESNET50_SVHN,\n",
    "    },\n",
    "    'Cifar100': {\n",
    "        'Cifar100_VGG16': get_VGG16_CIFAR100,\n",
    "        'Cifar100_RESNET50': get_RESNET50_CIFAR100,\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "clean_folder = model_folder/'clean'\n",
    "poisoned_folder = model_folder/'budget-10000'/'seed-42'\n",
    "first_unlearn_folder = model_folder/'budget-10000'/'seed-42'/'first-order'\n",
    "second_unlearn_folder = model_folder/'budget-10000'/'seed-42'/'second-order'\n",
    "\n",
    "# Run evaluation with load_and_evaluate_models\n",
    "results = load_and_evaluate_models(datasets, models, clean_folder, first_unlearn_folder)\n",
    "\n",
    "# Print final results\n",
    "for dataset_name, dataset_results in results.items():\n",
    "    for model_name, model_results in dataset_results.items():\n",
    "        print(f\"{dataset_name} - {model_name}:\")\n",
    "        print(f\"  Clean Accuracy: {model_results['clean_accuracy']:.4f}\")\n",
    "        print(f\"  Unlearned Accuracy: {model_results['unlearned_accuracy']:.4f}\")\n",
    "        print(f\"  Forget Score: {model_results['forget_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import os\n",
    "\n",
    "# results = {\n",
    "#     'clean': {},\n",
    "#     'poisoned': {},\n",
    "#     'first_order_unlearning': {},\n",
    "#     'second_order_unlearning': {}\n",
    "# }\n",
    "\n",
    "# update_targets = ['feature_extractor', 'classifier']\n",
    "# for dataset in datasets:\n",
    "#     for modelname in modelnames:\n",
    "#         for update_target in update_targets:\n",
    "#             print(f\"* First-order unlearning {modelname} on {dataset} poisoned model *\")\n",
    "#             fo_experiment(poisoned_folder/'first-order', train_kwargs, poison_kwargs, fo_unlearn_kwargs, dataset=dataset, modelname=modelname, update_target=update_target)\n",
    "#             print(f\" * Second-order unlearning {modelname} on {dataset} poisoned model *\")  \n",
    "#             so_experiment(poisoned_folder/'second-order', train_kwargs, poison_kwargs, so_unlearn_kwargs, dataset=dataset, modelname=modelname, update_target=update_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from None\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_35 (Conv2D)          (None, 32, 32, 64)        1792      \n",
      "                                                                 \n",
      " batch_normalization_45 (Bat  (None, 32, 32, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_35 (LeakyReLU)  (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " conv2d_36 (Conv2D)          (None, 32, 32, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_46 (Bat  (None, 32, 32, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_36 (LeakyReLU)  (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPoolin  (None, 16, 16, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " conv2d_37 (Conv2D)          (None, 16, 16, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_47 (Bat  (None, 16, 16, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_37 (LeakyReLU)  (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_38 (Conv2D)          (None, 16, 16, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_48 (Bat  (None, 16, 16, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_38 (LeakyReLU)  (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " max_pooling2d_16 (MaxPoolin  (None, 8, 8, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv2d_39 (Conv2D)          (None, 8, 8, 256)         295168    \n",
      "                                                                 \n",
      " batch_normalization_49 (Bat  (None, 8, 8, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_39 (LeakyReLU)  (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " conv2d_40 (Conv2D)          (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " batch_normalization_50 (Bat  (None, 8, 8, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_40 (LeakyReLU)  (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " conv2d_41 (Conv2D)          (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " batch_normalization_51 (Bat  (None, 8, 8, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_41 (LeakyReLU)  (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPoolin  (None, 4, 4, 256)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 4096)              16781312  \n",
      "                                                                 \n",
      " batch_normalization_52 (Bat  (None, 4096)             16384     \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_10 (ReLU)             (None, 4096)              0         \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 4096)              16781312  \n",
      "                                                                 \n",
      " batch_normalization_53 (Bat  (None, 4096)             16384     \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " re_lu_11 (ReLU)             (None, 4096)              0         \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 10)                40970     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35,376,458\n",
      "Trainable params: 35,357,770\n",
      "Non-trainable params: 18,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from Applications.Poisoning.model import get_VGG16_CIFAR10\n",
    "from tensorflow.keras.models import clone_model\n",
    "\n",
    "clean_model = get_VGG16_CIFAR10()\n",
    "clean_model.load_weights(clean_folder/'Cifar10_VGG16_best_model.hdf5')\n",
    "\n",
    "fo_model = clone_model(clean_model)\n",
    "so_model = clone_model(clean_model)\n",
    "clean_folder = BASE_DIR/'models'/'poisoning'/'clean'\n",
    "\n",
    "\n",
    "# fo_experiment(poisoned_folder/'first-order', train_kwargs, poison_kwargs, fo_unlearn_kwargs, dataset=\"Cifar10\", modelname='VGG16')\n",
    "\n",
    "fo_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No such layer: dense_2. Existing layers are [<keras.engine.input_layer.InputLayer object at 0x748c256a1c50>, <keras.layers.convolutional.Conv2D object at 0x748c256a1f50>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x748c256ab790>, <keras.layers.advanced_activations.LeakyReLU object at 0x748c256abdd0>, <keras.layers.convolutional.Conv2D object at 0x748c256abe90>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x748c256abe50>, <keras.layers.advanced_activations.LeakyReLU object at 0x748c256aec10>, <keras.layers.pooling.MaxPooling2D object at 0x748c256aeed0>, <keras.layers.core.dropout.Dropout object at 0x748c256b1250>, <keras.layers.convolutional.Conv2D object at 0x748c256ae550>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x748c256b19d0>, <keras.layers.advanced_activations.LeakyReLU object at 0x748c256b1fd0>, <keras.layers.convolutional.Conv2D object at 0x748c256b8350>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x748c256b8850>, <keras.layers.advanced_activations.LeakyReLU object at 0x748c256b8ed0>, <keras.layers.pooling.MaxPooling2D object at 0x748c256bb1d0>, <keras.layers.core.dropout.Dropout object at 0x748c256bb510>, <keras.layers.convolutional.Conv2D object at 0x748c256bb5d0>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x748c256bbcd0>, <keras.layers.advanced_activations.LeakyReLU object at 0x748c256bb690>, <keras.layers.convolutional.Conv2D object at 0x748c251c2650>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x748c251c2b50>, <keras.layers.advanced_activations.LeakyReLU object at 0x748c251c2450>, <keras.layers.convolutional.Conv2D object at 0x748c251cd4d0>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x748c251cd9d0>, <keras.layers.advanced_activations.LeakyReLU object at 0x748c251cdfd0>, <keras.layers.pooling.MaxPooling2D object at 0x748c251d1350>, <keras.layers.core.dropout.Dropout object at 0x748c25693b50>, <keras.layers.core.flatten.Flatten object at 0x748c251d1810>, <keras.layers.core.dense.Dense object at 0x748c251d19d0>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x748c251d1fd0>, <keras.layers.advanced_activations.ReLU object at 0x748c251d8510>, <keras.layers.core.dropout.Dropout object at 0x748c251d85d0>, <keras.layers.core.dense.Dense object at 0x748c256ab450>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x748c251d8e10>, <keras.layers.advanced_activations.ReLU object at 0x748c251d8590>, <keras.layers.core.dropout.Dropout object at 0x748c251df390>, <keras.layers.core.dense.Dense object at 0x748c251df350>].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_150629/2143180289.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mf10\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfo_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m're_lu_11'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mf11\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfo_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dropout_29'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfo_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dense_2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Create a new model with only the classifier part\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mget_layer\u001b[0;34m(self, name, index)\u001b[0m\n\u001b[1;32m   2629\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2631\u001b[0;31m       raise ValueError(f'No such layer: {name}. Existing layers are '\n\u001b[0m\u001b[1;32m   2632\u001b[0m                        f'{self.layers}.')\n\u001b[1;32m   2633\u001b[0m     raise ValueError('Provide either a layer name or layer index at '\n",
      "\u001b[0;31mValueError\u001b[0m: No such layer: dense_2. Existing layers are [<keras.engine.input_layer.InputLayer object at 0x748c256a1c50>, <keras.layers.convolutional.Conv2D object at 0x748c256a1f50>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x748c256ab790>, <keras.layers.advanced_activations.LeakyReLU object at 0x748c256abdd0>, <keras.layers.convolutional.Conv2D object at 0x748c256abe90>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x748c256abe50>, <keras.layers.advanced_activations.LeakyReLU object at 0x748c256aec10>, <keras.layers.pooling.MaxPooling2D object at 0x748c256aeed0>, <keras.layers.core.dropout.Dropout object at 0x748c256b1250>, <keras.layers.convolutional.Conv2D object at 0x748c256ae550>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x748c256b19d0>, <keras.layers.advanced_activations.LeakyReLU object at 0x748c256b1fd0>, <keras.layers.convolutional.Conv2D object at 0x748c256b8350>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x748c256b8850>, <keras.layers.advanced_activations.LeakyReLU object at 0x748c256b8ed0>, <keras.layers.pooling.MaxPooling2D object at 0x748c256bb1d0>, <keras.layers.core.dropout.Dropout object at 0x748c256bb510>, <keras.layers.convolutional.Conv2D object at 0x748c256bb5d0>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x748c256bbcd0>, <keras.layers.advanced_activations.LeakyReLU object at 0x748c256bb690>, <keras.layers.convolutional.Conv2D object at 0x748c251c2650>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x748c251c2b50>, <keras.layers.advanced_activations.LeakyReLU object at 0x748c251c2450>, <keras.layers.convolutional.Conv2D object at 0x748c251cd4d0>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x748c251cd9d0>, <keras.layers.advanced_activations.LeakyReLU object at 0x748c251cdfd0>, <keras.layers.pooling.MaxPooling2D object at 0x748c251d1350>, <keras.layers.core.dropout.Dropout object at 0x748c25693b50>, <keras.layers.core.flatten.Flatten object at 0x748c251d1810>, <keras.layers.core.dense.Dense object at 0x748c251d19d0>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x748c251d1fd0>, <keras.layers.advanced_activations.ReLU object at 0x748c251d8510>, <keras.layers.core.dropout.Dropout object at 0x748c251d85d0>, <keras.layers.core.dense.Dense object at 0x748c256ab450>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x748c251d8e10>, <keras.layers.advanced_activations.ReLU object at 0x748c251d8590>, <keras.layers.core.dropout.Dropout object at 0x748c251df390>, <keras.layers.core.dense.Dense object at 0x748c251df350>]."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "fo_fe = Model(inputs=fo_model.input, outputs=fo_model.get_layer('dropout_27').output)\n",
    "\n",
    "# classifier_input = fo_model.get_layer('flatten_2').output\n",
    "# fc1 = fo_model.get_layer('dense_24')(classifier_input)\n",
    "# fc2 = fo_model.get_layer('batch_normalization_79')(fc1)\n",
    "classifier_input = fo_model.get_layer('flatten_5').output\n",
    "f4 = fo_model.get_layer('dense_15')(classifier_input)\n",
    "f5 = fo_model.get_layer('batch_normalization_52')(f4)\n",
    "f6 = fo_model.get_layer('re_lu_10')(f5)\n",
    "f7 = fo_model.get_layer('dropout_28')(f6)\n",
    "f8 = fo_model.get_layer('dense_16')(f7)\n",
    "f9 = fo_model.get_layer('batch_normalization_53')(f8)\n",
    "f10 = fo_model.get_layer('re_lu_11')(f9)\n",
    "f11 = fo_model.get_layer('dropout_29')(f10)\n",
    "predictions = fo_model.get_layer('dense_2')(f11)\n",
    "\n",
    "# Create a new model with only the classifier part\n",
    "fo_cl = Model(inputs=classifier_input, outputs=predictions)\n",
    "\n",
    "combined_model_fo = Model(inputs=fo_fe.input, outputs=predictions)\n",
    "\n",
    "combined_model_fo.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No such layer: dropout_2. Existing layers are [<keras.engine.input_layer.InputLayer object at 0x748c256a1c50>, <keras.layers.convolutional.Conv2D object at 0x748c256a1f50>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x748c256ab790>, <keras.layers.advanced_activations.LeakyReLU object at 0x748c256abdd0>, <keras.layers.convolutional.Conv2D object at 0x748c256abe90>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x748c256abe50>, <keras.layers.advanced_activations.LeakyReLU object at 0x748c256aec10>, <keras.layers.pooling.MaxPooling2D object at 0x748c256aeed0>, <keras.layers.core.dropout.Dropout object at 0x748c256b1250>, <keras.layers.convolutional.Conv2D object at 0x748c256ae550>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x748c256b19d0>, <keras.layers.advanced_activations.LeakyReLU object at 0x748c256b1fd0>, <keras.layers.convolutional.Conv2D object at 0x748c256b8350>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x748c256b8850>, <keras.layers.advanced_activations.LeakyReLU object at 0x748c256b8ed0>, <keras.layers.pooling.MaxPooling2D object at 0x748c256bb1d0>, <keras.layers.core.dropout.Dropout object at 0x748c256bb510>, <keras.layers.convolutional.Conv2D object at 0x748c256bb5d0>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x748c256bbcd0>, <keras.layers.advanced_activations.LeakyReLU object at 0x748c256bb690>, <keras.layers.convolutional.Conv2D object at 0x748c251c2650>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x748c251c2b50>, <keras.layers.advanced_activations.LeakyReLU object at 0x748c251c2450>, <keras.layers.convolutional.Conv2D object at 0x748c251cd4d0>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x748c251cd9d0>, <keras.layers.advanced_activations.LeakyReLU object at 0x748c251cdfd0>, <keras.layers.pooling.MaxPooling2D object at 0x748c251d1350>, <keras.layers.core.dropout.Dropout object at 0x748c25693b50>, <keras.layers.core.flatten.Flatten object at 0x748c251d1810>, <keras.layers.core.dense.Dense object at 0x748c251d19d0>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x748c251d1fd0>, <keras.layers.advanced_activations.ReLU object at 0x748c251d8510>, <keras.layers.core.dropout.Dropout object at 0x748c251d85d0>, <keras.layers.core.dense.Dense object at 0x748c256ab450>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x748c251d8e10>, <keras.layers.advanced_activations.ReLU object at 0x748c251d8590>, <keras.layers.core.dropout.Dropout object at 0x748c251df390>, <keras.layers.core.dense.Dense object at 0x748c251df350>].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_150629/2104945993.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# fo_fe.summary()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# fo_cl.summary()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mso_fe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfo_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfo_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dropout_2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# classifier_input = fo_model.get_layer('flatten_2').output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mget_layer\u001b[0;34m(self, name, index)\u001b[0m\n\u001b[1;32m   2629\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2631\u001b[0;31m       raise ValueError(f'No such layer: {name}. Existing layers are '\n\u001b[0m\u001b[1;32m   2632\u001b[0m                        f'{self.layers}.')\n\u001b[1;32m   2633\u001b[0m     raise ValueError('Provide either a layer name or layer index at '\n",
      "\u001b[0;31mValueError\u001b[0m: No such layer: dropout_2. Existing layers are [<keras.engine.input_layer.InputLayer object at 0x748c256a1c50>, <keras.layers.convolutional.Conv2D object at 0x748c256a1f50>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x748c256ab790>, <keras.layers.advanced_activations.LeakyReLU object at 0x748c256abdd0>, <keras.layers.convolutional.Conv2D object at 0x748c256abe90>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x748c256abe50>, <keras.layers.advanced_activations.LeakyReLU object at 0x748c256aec10>, <keras.layers.pooling.MaxPooling2D object at 0x748c256aeed0>, <keras.layers.core.dropout.Dropout object at 0x748c256b1250>, <keras.layers.convolutional.Conv2D object at 0x748c256ae550>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x748c256b19d0>, <keras.layers.advanced_activations.LeakyReLU object at 0x748c256b1fd0>, <keras.layers.convolutional.Conv2D object at 0x748c256b8350>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x748c256b8850>, <keras.layers.advanced_activations.LeakyReLU object at 0x748c256b8ed0>, <keras.layers.pooling.MaxPooling2D object at 0x748c256bb1d0>, <keras.layers.core.dropout.Dropout object at 0x748c256bb510>, <keras.layers.convolutional.Conv2D object at 0x748c256bb5d0>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x748c256bbcd0>, <keras.layers.advanced_activations.LeakyReLU object at 0x748c256bb690>, <keras.layers.convolutional.Conv2D object at 0x748c251c2650>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x748c251c2b50>, <keras.layers.advanced_activations.LeakyReLU object at 0x748c251c2450>, <keras.layers.convolutional.Conv2D object at 0x748c251cd4d0>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x748c251cd9d0>, <keras.layers.advanced_activations.LeakyReLU object at 0x748c251cdfd0>, <keras.layers.pooling.MaxPooling2D object at 0x748c251d1350>, <keras.layers.core.dropout.Dropout object at 0x748c25693b50>, <keras.layers.core.flatten.Flatten object at 0x748c251d1810>, <keras.layers.core.dense.Dense object at 0x748c251d19d0>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x748c251d1fd0>, <keras.layers.advanced_activations.ReLU object at 0x748c251d8510>, <keras.layers.core.dropout.Dropout object at 0x748c251d85d0>, <keras.layers.core.dense.Dense object at 0x748c256ab450>, <keras.layers.normalization.batch_normalization.BatchNormalization object at 0x748c251d8e10>, <keras.layers.advanced_activations.ReLU object at 0x748c251d8590>, <keras.layers.core.dropout.Dropout object at 0x748c251df390>, <keras.layers.core.dense.Dense object at 0x748c251df350>]."
     ]
    }
   ],
   "source": [
    "# fo_fe.summary()\n",
    "# fo_cl.summary()\n",
    "so_fe = Model(inputs=fo_model.input, outputs=fo_model.get_layer('dropout_2').output)\n",
    "\n",
    "# classifier_input = fo_model.get_layer('flatten_2').output\n",
    "# fc1 = fo_model.get_layer('dense_24')(classifier_input)\n",
    "# fc2 = fo_model.get_layer('batch_normalization_79')(fc1)\n",
    "classifier_input = so_model.get_layer('flatten_5').output\n",
    "f4 = so_model.get_layer('dense_15')(classifier_input)\n",
    "f5 = so_model.get_layer('batch_normalization_52')(f4)\n",
    "f6 = so_model.get_layer('re_lu_10')(f5)\n",
    "f7 = so_model.get_layer('dropout_28')(f6)\n",
    "f8 = so_model.get_layer('dense_16')(f7)\n",
    "f9 = so_model.get_layer('batch_normalization_53')(f8)\n",
    "f10 = so_model.get_layer('re_lu_11')(f9)\n",
    "f11 = so_model.get_layer('dropout_29')(f10)\n",
    "predictions = so_model.get_layer('dense_2')(f11)\n",
    "\n",
    "combined_model_so = Model(inputs=so_fe.input, outputs=predictions)\n",
    "\n",
    "# combined_model_so.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import dirname as parent\n",
    "from Applications.Poisoning.poison.injector import LabelflipInjector\n",
    "from Applications.Poisoning.dataset import Cifar10, Mnist, SVHN, Cifar100\n",
    "from util import UnlearningResult, reduce_dataset\n",
    "from Applications.Poisoning.unlearn.common import evaluate_unlearning, get_delta_idx, batch_pred, unlearn_update, evaluate_model_diff\n",
    "import json\n",
    "from util import LoggedGradientTape, ModelTmpState, CSVLogger, measure_time, GradientLoggingContext\n",
    "\n",
    "\n",
    "# modeltype = extractfeatures_VGG16\n",
    "def first_unlearning(model=None):\n",
    "    data = Cifar10.load()\n",
    "    model = fo_model\n",
    "    (x_train, y_train), _, _ = data\n",
    "    y_train_orig = y_train.copy()\n",
    "\n",
    "    injector_path = os.path.join(model_folder, 'injector.pkl')\n",
    "    if os.path.exists(injector_path):\n",
    "        injector = LabelflipInjector.from_pickle(injector_path)\n",
    "    else:\n",
    "        injector = LabelflipInjector(parent(model_folder), **poison_kwargs)\n",
    "        x_train, y_train = injector.inject(x_train, y_train)\n",
    "        data = ((x_train, y_train), data[1], data[2])\n",
    "\n",
    "    # poisoned_folder/'first-order'\n",
    "    # prepare unlearning data\n",
    "    reduction = 1.0\n",
    "    (x_train,  y_train), _, _ = data\n",
    "    x_train, y_train, idx_reduced, delta_idx = reduce_dataset( x_train, y_train, reduction=reduction, delta_idx=injector.injected_idx)\n",
    "    print(f\">> reduction={reduction}, new train size: {x_train.shape[0]}\")\n",
    "    y_train_orig = y_train_orig[idx_reduced]\n",
    "    delta_idx = injector.injected_idx\n",
    "    data = ((x_train, y_train), data[1], data[2])\n",
    "\n",
    "    # Get clean accuracy\n",
    "    with open(model_folder/'clean'/'Cifar10_extractfeatures_VGG16_train_results.json', 'r') as f:\n",
    "        clean_acc = json.load(f)['accuracy']\n",
    "\n",
    "    from tensorflow.keras.backend import clear_session  \n",
    "    from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "    clear_session()\n",
    "\n",
    "    (x_train, y_train), (x_test, y_test), (x_valid, y_valid) = data\n",
    "    params = np.sum(np.product([xi for xi in x.shape]) for x in model.trainable_variables).item()\n",
    "    print(f'Nb params : {params}')\n",
    "\n",
    "    new_theta, diverged, logs, duration_s = unlearn_update(\n",
    "        x_train, y_train, y_train_orig, delta_idx, model, x_valid, y_valid, fo_unlearn_kwargs, verbose=0, cm_dir=None, log_dir=None)\n",
    "\n",
    "    new_model = model\n",
    "    new_model.set_weights(new_theta)\n",
    "    new_model.save_weights('cifar10_vgg16_repaired.hdf5')\n",
    "\n",
    "    acc_before, acc_after, diverged = evaluate_model_diff(\n",
    "        model, new_model, x_valid, y_valid, diverged, 0, clean_acc)\n",
    "    print(f'Accuracy before unlearning: {acc_before:.2f}')\n",
    "    print(f'Accuracy after unlearning: {acc_after:.2f}')\n",
    "    return new_model\n",
    "\n",
    "from Applications.Poisoning.unlearn.core import get_gradients_diff, get_inv_hvp_lissa\n",
    "\n",
    "def approx_retraining(model, z_x, z_y, z_x_delta, z_y_delta, order=2, hvp_x=None, hvp_y=None):\n",
    "    if order == 1:\n",
    "        tau = fo_unlearn_kwargs.get('tau', 1)\n",
    "\n",
    "        # first order update\n",
    "        diff = get_gradients_diff(model, z_x, z_y, z_x_delta, z_y_delta)\n",
    "        d_theta = diff\n",
    "        diverged = False\n",
    "    elif order == 2:\n",
    "        tau = 1  # tau not used by second-order\n",
    "\n",
    "        # second order update\n",
    "        diff = get_gradients_diff(model, z_x, z_y, z_x_delta, z_y_delta)\n",
    "        # skip hvp if diff == 0\n",
    "        if np.sum(np.sum(d) for d in diff) == 0:\n",
    "            d_theta = diff\n",
    "            diverged = False\n",
    "        # elif conjugate_gradients:\n",
    "        #     raise NotImplementedError('Conjugate Gradients is not implemented yet!')\n",
    "        else:\n",
    "            assert hvp_x is not None and hvp_y is not None\n",
    "            d_theta, diverged = get_inv_hvp_lissa(model, hvp_x, hvp_y, diff, verbose=0, hvp_logger=None, **fo_unlearn_kwargs)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> reduction=1.0, new train size: 50000\n",
      "Nb params : 35357770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/conite/.pyenv/versions/3.7.7/envs/ML_ENV/lib/python3.7/site-packages/ipykernel_launcher.py:46: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "\n",
    "fo_model.compile(optimizer=Adam(learning_rate=0.001, amsgrad=True),loss=categorical_crossentropy, metrics='accuracy')\n",
    "\n",
    "fo_unlearner = first_unlearning(model=fo_model)\n",
    "print(f'Unlearning with combined model')\n",
    "combined_model_fo_unlearner = first_unlearning(model=combined_model_fo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
